{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dyc2cIvDXzPn",
        "outputId": "d6e6f1b4-b2bb-43f9-883e-0b9e51e115d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: optuna in c:\\users\\pablo\\anaconda3\\envs\\proyecto_keepcoding\\lib\\site-packages (4.2.1)\n",
            "Collecting torch\n",
            "  Using cached torch-2.6.0-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\pablo\\anaconda3\\envs\\proyecto_keepcoding\\lib\\site-packages (from optuna) (1.15.1)\n",
            "Requirement already satisfied: colorlog in c:\\users\\pablo\\anaconda3\\envs\\proyecto_keepcoding\\lib\\site-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\pablo\\anaconda3\\envs\\proyecto_keepcoding\\lib\\site-packages (from optuna) (2.2.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\pablo\\anaconda3\\envs\\proyecto_keepcoding\\lib\\site-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\pablo\\anaconda3\\envs\\proyecto_keepcoding\\lib\\site-packages (from optuna) (2.0.39)\n",
            "Requirement already satisfied: tqdm in c:\\users\\pablo\\anaconda3\\envs\\proyecto_keepcoding\\lib\\site-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in c:\\users\\pablo\\anaconda3\\envs\\proyecto_keepcoding\\lib\\site-packages (from optuna) (6.0.2)\n",
            "Collecting filelock (from torch)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\pablo\\anaconda3\\envs\\proyecto_keepcoding\\lib\\site-packages (from torch) (4.12.2)\n",
            "Collecting networkx (from torch)\n",
            "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting sympy==1.13.1 (from torch)\n",
            "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: Mako in c:\\users\\pablo\\anaconda3\\envs\\proyecto_keepcoding\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\pablo\\anaconda3\\envs\\proyecto_keepcoding\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\pablo\\anaconda3\\envs\\proyecto_keepcoding\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pablo\\anaconda3\\envs\\proyecto_keepcoding\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
            "Using cached torch-2.6.0-cp311-cp311-win_amd64.whl (204.2 MB)\n",
            "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Installing collected packages: mpmath, sympy, networkx, jinja2, fsspec, filelock, torch\n",
            "Successfully installed filelock-3.18.0 fsspec-2025.3.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n6wCybfWXq4C"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Pablo\\anaconda3\\envs\\proyecto_keepcoding\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import optuna\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from utils import *\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3bp7lQuY4m-",
        "outputId": "1aeee262-993b-42cc-c828-61a70730878e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usando dispositivo: cpu\n"
          ]
        }
      ],
      "source": [
        "# Verificar si CUDA est√° disponible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "Cz2HisGsY5eE",
        "outputId": "64de76ba-ec02-46e6-d066-655fcb7a980c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PP</th>\n",
              "      <th>Blok</th>\n",
              "      <th>Condition</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>Valence_rc</th>\n",
              "      <th>Arousal_rc</th>\n",
              "      <th>Dominance</th>\n",
              "      <th>Stress</th>\n",
              "      <th>MentalEffort</th>\n",
              "      <th>MentalDemand</th>\n",
              "      <th>...</th>\n",
              "      <th>ShoulderRight_ElbowRightPlaneZXAxisXstdv</th>\n",
              "      <th>ShoulderRight_ElbowRightPlaneXYAxisYstdv</th>\n",
              "      <th>ShoulderRight_ElbowRightPlaneYZAxisZstdv</th>\n",
              "      <th>ElbowRight_WristRightPlaneZXAxisXstdv</th>\n",
              "      <th>ElbowRight_WristRightPlaneXYAxisYstdv</th>\n",
              "      <th>ElbowRight_WristRightPlaneYZAxisZstdv</th>\n",
              "      <th>WristRight_HandRightPlaneZXAxisXstdv</th>\n",
              "      <th>WristRight_HandRightPlaneXYAxisYstdv</th>\n",
              "      <th>WristRight_HandRightKinectZAxisstdv</th>\n",
              "      <th>momento_dia</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PP1</td>\n",
              "      <td>1</td>\n",
              "      <td>R</td>\n",
              "      <td>2012-09-18 13:16:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>22.946337</td>\n",
              "      <td>5.605230</td>\n",
              "      <td>13.237048</td>\n",
              "      <td>30.932939</td>\n",
              "      <td>102.460262</td>\n",
              "      <td>8.444481</td>\n",
              "      <td>54.436879</td>\n",
              "      <td>75.886750</td>\n",
              "      <td>155.734343</td>\n",
              "      <td>tarde</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PP1</td>\n",
              "      <td>1</td>\n",
              "      <td>R</td>\n",
              "      <td>2012-09-18 13:17:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>7.194860</td>\n",
              "      <td>2.468881</td>\n",
              "      <td>3.886001</td>\n",
              "      <td>2.000359</td>\n",
              "      <td>3.431337</td>\n",
              "      <td>1.160298</td>\n",
              "      <td>4.579341</td>\n",
              "      <td>2.868843</td>\n",
              "      <td>171.486911</td>\n",
              "      <td>tarde</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PP1</td>\n",
              "      <td>1</td>\n",
              "      <td>R</td>\n",
              "      <td>2012-09-18 13:18:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>14.726365</td>\n",
              "      <td>2.953021</td>\n",
              "      <td>9.185623</td>\n",
              "      <td>13.379140</td>\n",
              "      <td>48.162012</td>\n",
              "      <td>30.508877</td>\n",
              "      <td>23.013755</td>\n",
              "      <td>30.629061</td>\n",
              "      <td>149.748809</td>\n",
              "      <td>tarde</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PP1</td>\n",
              "      <td>1</td>\n",
              "      <td>R</td>\n",
              "      <td>2012-09-18 13:19:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>2.542686</td>\n",
              "      <td>1.564318</td>\n",
              "      <td>2.453674</td>\n",
              "      <td>4.796273</td>\n",
              "      <td>8.223865</td>\n",
              "      <td>1.114906</td>\n",
              "      <td>3.854074</td>\n",
              "      <td>4.048611</td>\n",
              "      <td>173.149430</td>\n",
              "      <td>tarde</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PP1</td>\n",
              "      <td>1</td>\n",
              "      <td>R</td>\n",
              "      <td>2012-09-18 13:20:00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>9.905241</td>\n",
              "      <td>20.342488</td>\n",
              "      <td>5.552234</td>\n",
              "      <td>7.198096</td>\n",
              "      <td>22.942488</td>\n",
              "      <td>46.386278</td>\n",
              "      <td>17.514534</td>\n",
              "      <td>29.702675</td>\n",
              "      <td>165.368301</td>\n",
              "      <td>tarde</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3134</th>\n",
              "      <td>PP25</td>\n",
              "      <td>3</td>\n",
              "      <td>T</td>\n",
              "      <td>2012-11-07 16:15:00</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>2.7</td>\n",
              "      <td>3.6</td>\n",
              "      <td>...</td>\n",
              "      <td>36.355940</td>\n",
              "      <td>5.323538</td>\n",
              "      <td>17.908446</td>\n",
              "      <td>56.076888</td>\n",
              "      <td>124.607529</td>\n",
              "      <td>15.482486</td>\n",
              "      <td>87.838259</td>\n",
              "      <td>158.284130</td>\n",
              "      <td>38.905423</td>\n",
              "      <td>tarde</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3135</th>\n",
              "      <td>PP25</td>\n",
              "      <td>3</td>\n",
              "      <td>T</td>\n",
              "      <td>2012-11-07 16:16:00</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>2.7</td>\n",
              "      <td>3.6</td>\n",
              "      <td>...</td>\n",
              "      <td>14.714479</td>\n",
              "      <td>1.848907</td>\n",
              "      <td>3.908359</td>\n",
              "      <td>34.878709</td>\n",
              "      <td>159.984897</td>\n",
              "      <td>5.233569</td>\n",
              "      <td>63.512098</td>\n",
              "      <td>164.607803</td>\n",
              "      <td>27.895148</td>\n",
              "      <td>tarde</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3136</th>\n",
              "      <td>PP25</td>\n",
              "      <td>3</td>\n",
              "      <td>T</td>\n",
              "      <td>2012-11-07 16:17:00</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>2.7</td>\n",
              "      <td>3.6</td>\n",
              "      <td>...</td>\n",
              "      <td>22.742133</td>\n",
              "      <td>3.048023</td>\n",
              "      <td>6.468803</td>\n",
              "      <td>53.988308</td>\n",
              "      <td>169.548392</td>\n",
              "      <td>10.757358</td>\n",
              "      <td>60.488127</td>\n",
              "      <td>142.557094</td>\n",
              "      <td>21.761699</td>\n",
              "      <td>tarde</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3137</th>\n",
              "      <td>PP25</td>\n",
              "      <td>3</td>\n",
              "      <td>T</td>\n",
              "      <td>2012-11-07 16:18:00</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>2.7</td>\n",
              "      <td>3.6</td>\n",
              "      <td>...</td>\n",
              "      <td>28.595668</td>\n",
              "      <td>27.981837</td>\n",
              "      <td>8.424373</td>\n",
              "      <td>43.183524</td>\n",
              "      <td>55.599939</td>\n",
              "      <td>14.620391</td>\n",
              "      <td>72.590531</td>\n",
              "      <td>163.087296</td>\n",
              "      <td>28.944050</td>\n",
              "      <td>tarde</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3138</th>\n",
              "      <td>PP25</td>\n",
              "      <td>3</td>\n",
              "      <td>T</td>\n",
              "      <td>2012-11-07 16:19:00</td>\n",
              "      <td>9.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>2.7</td>\n",
              "      <td>3.6</td>\n",
              "      <td>...</td>\n",
              "      <td>24.310952</td>\n",
              "      <td>3.004678</td>\n",
              "      <td>6.545043</td>\n",
              "      <td>48.474069</td>\n",
              "      <td>55.974389</td>\n",
              "      <td>14.594551</td>\n",
              "      <td>73.517789</td>\n",
              "      <td>155.022667</td>\n",
              "      <td>25.972857</td>\n",
              "      <td>tarde</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3139 rows √ó 173 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        PP  Blok Condition           timestamp  Valence_rc  Arousal_rc  \\\n",
              "0      PP1     1         R 2012-09-18 13:16:00         NaN         NaN   \n",
              "1      PP1     1         R 2012-09-18 13:17:00         NaN         NaN   \n",
              "2      PP1     1         R 2012-09-18 13:18:00         NaN         NaN   \n",
              "3      PP1     1         R 2012-09-18 13:19:00         NaN         NaN   \n",
              "4      PP1     1         R 2012-09-18 13:20:00         NaN         NaN   \n",
              "...    ...   ...       ...                 ...         ...         ...   \n",
              "3134  PP25     3         T 2012-11-07 16:15:00         9.0         8.0   \n",
              "3135  PP25     3         T 2012-11-07 16:16:00         9.0         8.0   \n",
              "3136  PP25     3         T 2012-11-07 16:17:00         9.0         8.0   \n",
              "3137  PP25     3         T 2012-11-07 16:18:00         9.0         8.0   \n",
              "3138  PP25     3         T 2012-11-07 16:19:00         9.0         8.0   \n",
              "\n",
              "      Dominance  Stress  MentalEffort  MentalDemand  ...  \\\n",
              "0           NaN     NaN           NaN           NaN  ...   \n",
              "1           NaN     NaN           NaN           NaN  ...   \n",
              "2           NaN     NaN           NaN           NaN  ...   \n",
              "3           NaN     NaN           NaN           NaN  ...   \n",
              "4           NaN     NaN           NaN           NaN  ...   \n",
              "...         ...     ...           ...           ...  ...   \n",
              "3134        9.0     0.3           2.7           3.6  ...   \n",
              "3135        9.0     0.3           2.7           3.6  ...   \n",
              "3136        9.0     0.3           2.7           3.6  ...   \n",
              "3137        9.0     0.3           2.7           3.6  ...   \n",
              "3138        9.0     0.3           2.7           3.6  ...   \n",
              "\n",
              "      ShoulderRight_ElbowRightPlaneZXAxisXstdv  \\\n",
              "0                                    22.946337   \n",
              "1                                     7.194860   \n",
              "2                                    14.726365   \n",
              "3                                     2.542686   \n",
              "4                                     9.905241   \n",
              "...                                        ...   \n",
              "3134                                 36.355940   \n",
              "3135                                 14.714479   \n",
              "3136                                 22.742133   \n",
              "3137                                 28.595668   \n",
              "3138                                 24.310952   \n",
              "\n",
              "      ShoulderRight_ElbowRightPlaneXYAxisYstdv  \\\n",
              "0                                     5.605230   \n",
              "1                                     2.468881   \n",
              "2                                     2.953021   \n",
              "3                                     1.564318   \n",
              "4                                    20.342488   \n",
              "...                                        ...   \n",
              "3134                                  5.323538   \n",
              "3135                                  1.848907   \n",
              "3136                                  3.048023   \n",
              "3137                                 27.981837   \n",
              "3138                                  3.004678   \n",
              "\n",
              "      ShoulderRight_ElbowRightPlaneYZAxisZstdv  \\\n",
              "0                                    13.237048   \n",
              "1                                     3.886001   \n",
              "2                                     9.185623   \n",
              "3                                     2.453674   \n",
              "4                                     5.552234   \n",
              "...                                        ...   \n",
              "3134                                 17.908446   \n",
              "3135                                  3.908359   \n",
              "3136                                  6.468803   \n",
              "3137                                  8.424373   \n",
              "3138                                  6.545043   \n",
              "\n",
              "      ElbowRight_WristRightPlaneZXAxisXstdv  \\\n",
              "0                                 30.932939   \n",
              "1                                  2.000359   \n",
              "2                                 13.379140   \n",
              "3                                  4.796273   \n",
              "4                                  7.198096   \n",
              "...                                     ...   \n",
              "3134                              56.076888   \n",
              "3135                              34.878709   \n",
              "3136                              53.988308   \n",
              "3137                              43.183524   \n",
              "3138                              48.474069   \n",
              "\n",
              "      ElbowRight_WristRightPlaneXYAxisYstdv  \\\n",
              "0                                102.460262   \n",
              "1                                  3.431337   \n",
              "2                                 48.162012   \n",
              "3                                  8.223865   \n",
              "4                                 22.942488   \n",
              "...                                     ...   \n",
              "3134                             124.607529   \n",
              "3135                             159.984897   \n",
              "3136                             169.548392   \n",
              "3137                              55.599939   \n",
              "3138                              55.974389   \n",
              "\n",
              "      ElbowRight_WristRightPlaneYZAxisZstdv  \\\n",
              "0                                  8.444481   \n",
              "1                                  1.160298   \n",
              "2                                 30.508877   \n",
              "3                                  1.114906   \n",
              "4                                 46.386278   \n",
              "...                                     ...   \n",
              "3134                              15.482486   \n",
              "3135                               5.233569   \n",
              "3136                              10.757358   \n",
              "3137                              14.620391   \n",
              "3138                              14.594551   \n",
              "\n",
              "      WristRight_HandRightPlaneZXAxisXstdv  \\\n",
              "0                                54.436879   \n",
              "1                                 4.579341   \n",
              "2                                23.013755   \n",
              "3                                 3.854074   \n",
              "4                                17.514534   \n",
              "...                                    ...   \n",
              "3134                             87.838259   \n",
              "3135                             63.512098   \n",
              "3136                             60.488127   \n",
              "3137                             72.590531   \n",
              "3138                             73.517789   \n",
              "\n",
              "      WristRight_HandRightPlaneXYAxisYstdv  \\\n",
              "0                                75.886750   \n",
              "1                                 2.868843   \n",
              "2                                30.629061   \n",
              "3                                 4.048611   \n",
              "4                                29.702675   \n",
              "...                                    ...   \n",
              "3134                            158.284130   \n",
              "3135                            164.607803   \n",
              "3136                            142.557094   \n",
              "3137                            163.087296   \n",
              "3138                            155.022667   \n",
              "\n",
              "      WristRight_HandRightKinectZAxisstdv  momento_dia  \n",
              "0                              155.734343        tarde  \n",
              "1                              171.486911        tarde  \n",
              "2                              149.748809        tarde  \n",
              "3                              173.149430        tarde  \n",
              "4                              165.368301        tarde  \n",
              "...                                   ...          ...  \n",
              "3134                            38.905423        tarde  \n",
              "3135                            27.895148        tarde  \n",
              "3136                            21.761699        tarde  \n",
              "3137                            28.944050        tarde  \n",
              "3138                            25.972857        tarde  \n",
              "\n",
              "[3139 rows x 173 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cargar y preprocesar datos\n",
        "df_estres = pd.read_csv('SWELLdata.csv', decimal=\",\", delimiter=\";\")\n",
        "\n",
        "formateo_fechas(df_estres)\n",
        "clasificar_momento_dia(df_estres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1gNUJTXY8sF",
        "outputId": "a3d14cec-525e-47bb-e62e-f83a0fd60ea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Se eliminaron las columnas: ['timestamp', 'HR', 'RMSSD', 'SCL', 'Valence_rc', 'Arousal_rc', 'Dominance', 'MentalEffort', 'MentalDemand', 'PhysicalDemand', 'TemporalDemand', 'Effort', 'Performance_rc', 'Frustration', 'NasaTLX', 'Squality', 'Sneutral', 'Shappy', 'Ssad', 'Sangry', 'Ssurprised', 'Sscared', 'Sdisgusted', 'Svalence']\n"
          ]
        }
      ],
      "source": [
        "# Eliminar columnas irrelevantes\n",
        "columnas_a_eliminar = ['timestamp', 'HR', 'RMSSD', 'SCL', 'Valence_rc', 'Arousal_rc',\n",
        "                        'Dominance', 'MentalEffort', 'MentalDemand', 'PhysicalDemand',\n",
        "                        'TemporalDemand', 'Effort', 'Performance_rc', 'Frustration',\n",
        "                        'NasaTLX', 'Squality', 'Sneutral', 'Shappy', 'Ssad', 'Sangry',\n",
        "                        'Ssurprised', 'Sscared', 'Sdisgusted', 'Svalence']\n",
        "df_estres = eliminar_columnas(df_estres, columnas_a_eliminar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6pahtWDPY_1t"
      },
      "outputs": [],
      "source": [
        "# Filtrar condici√≥n R y momento_d√≠a = noche\n",
        "df_estres = df_estres[df_estres['Condition'] != 'R']\n",
        "df_estres = df_estres[df_estres['momento_dia'] != 'noche']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YyuICiOkZEwQ"
      },
      "outputs": [],
      "source": [
        "# Reordenar columna 'momento_dia'\n",
        "cols = list(df_estres.columns)\n",
        "cols.pop(cols.index('momento_dia'))\n",
        "cols.insert(3, 'momento_dia')\n",
        "df_estres = df_estres[cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SJN9k9SUZII1"
      },
      "outputs": [],
      "source": [
        "# Dividir datos en Train (60%), Eval (20%) y Test (20%)\n",
        "X = df_estres.drop('Stress', axis=1)\n",
        "y = df_estres['Stress']\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, shuffle=True)\n",
        "X_eval, X_test, y_eval, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nTXt_HgAZLQF"
      },
      "outputs": [],
      "source": [
        "# Eliminar PP de cada conjunto\n",
        "for dataset in [X_train, X_eval, X_test]:\n",
        "    dataset.drop(columns=['PP'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xsLf_mwsZPZY"
      },
      "outputs": [],
      "source": [
        "# Aplicar la misma imputaci√≥n a cada dataset\n",
        "stats_X_train = X_train.describe().T[['mean', '50%']].assign(mode=X_train.mode().iloc[0])\n",
        "\n",
        "def imputar_valores(df, stats):\n",
        "    for col in df.columns:\n",
        "        if col in stats.index:\n",
        "            moda, mediana, media = stats.loc[col, ['mode', '50%', 'mean']]\n",
        "            df[col] = df[col].fillna(moda if moda != 0 else mediana if mediana != 0 else media)\n",
        "    return df\n",
        "\n",
        "X_train = imputar_valores(X_train, stats_X_train)\n",
        "X_eval = imputar_valores(X_eval, stats_X_train)\n",
        "X_test = imputar_valores(X_test, stats_X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "up_337rjZTWu"
      },
      "outputs": [],
      "source": [
        "# Convertir categ√≥ricas a tipo category y aplicar One-Hot Encoding\n",
        "for dataset in [X_train, X_eval, X_test]:\n",
        "    for col in ['Blok', 'Condition', 'momento_dia']:\n",
        "        dataset[col] = dataset[col].astype('category')\n",
        "    dataset = pd.get_dummies(dataset, columns=['Blok', 'Condition', 'momento_dia'])\n",
        "\n",
        "X_train = pd.get_dummies(X_train, columns=['Blok', 'Condition', 'momento_dia'])\n",
        "X_eval = pd.get_dummies(X_eval, columns=['Blok', 'Condition', 'momento_dia'])\n",
        "X_test = pd.get_dummies(X_test, columns=['Blok', 'Condition', 'momento_dia'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzGebIzFaAoF",
        "outputId": "8f7b8150-eb5b-4d22-cd0e-6947db0cd5b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eliminadas 23 columnas altamente correlacionadas: ['SAu25_LipsPart', 'Condition_N', 'ShoulderCenter_ShoulderRightPlaneYZAxisZavg', 'ShoulderCenter_ShoulderLeftShoulderLeft_ElbowLeftavg', 'Spine_ShoulderCenterPlaneYZAxisZavg', 'Spine_ShoulderCenterPlaneYZAxisZstdv', 'ShoulderRight_ElbowRightPlaneXYAxisYavg', 'Spine_ShoulderCenterShoulderCenter_ShoulderLeftavg', 'HipCenter_SpineSpine_ShoulderCenteravg', 'SnChars', 'HipCenter_SpinePlaneXYAxisYavg', 'Spine_ShoulderCenterPlaneXYAxisYstdv', 'HipCenter_SpinePlaneYZAxisZavg', 'ShoulderCenter_ShoulderRightPlaneXYAxisYavg', 'SnSpaces', 'Spine_ShoulderCenterPlaneXYAxisYavg', 'ShoulderCenter_ShoulderRightPlaneXYAxisYstdv', 'Spine_ShoulderCenterShoulderCenter_Headavg', 'ShoulderCenter_ShoulderLeftPlaneXYAxisYavg', 'momento_dia_tarde', 'ShoulderCenter_ShoulderRightPlaneZXAxisXavg', 'ShoulderCenter_ShoulderLeftPlaneXYAxisYstdv', 'ElbowLeft_WristLeftWristLeft_HandLeftavg']\n"
          ]
        }
      ],
      "source": [
        "# Eliminar columnas altamente correlacionadas (>0.9)\n",
        "corr_matrix = X_train.corr().abs()\n",
        "high_corr_cols = {corr_matrix.columns[i] for i in range(len(corr_matrix.columns))\n",
        "                  for j in range(i) if corr_matrix.iloc[i, j] > 0.9}\n",
        "\n",
        "X_train.drop(columns=high_corr_cols, inplace=True)\n",
        "X_eval.drop(columns=high_corr_cols, inplace=True)\n",
        "X_test.drop(columns=high_corr_cols, inplace=True)\n",
        "\n",
        "print(f\"Eliminadas {len(high_corr_cols)} columnas altamente correlacionadas: {list(high_corr_cols)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['SyHeadOrientation', 'SxHeadOrientation', 'SzHeadOrientation',\n",
              "       'SmouthOpen', 'SleftEyeClosed', 'SrightEyeClosed',\n",
              "       'SleftEyebrowLowered', 'SleftEyebrowRaised', 'SrightEyebrowLowered',\n",
              "       'SrightEyebrowRaised',\n",
              "       ...\n",
              "       'ElbowRight_WristRightPlaneYZAxisZstdv',\n",
              "       'WristRight_HandRightPlaneZXAxisXstdv',\n",
              "       'WristRight_HandRightPlaneXYAxisYstdv',\n",
              "       'WristRight_HandRightKinectZAxisstdv', 'Blok_1', 'Blok_2', 'Blok_3',\n",
              "       'Condition_I', 'Condition_T', 'momento_dia_ma√±ana'],\n",
              "      dtype='object', length=129)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_columns = X_train.columns\n",
        "X_train_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rOWuWgSAaIjk"
      },
      "outputs": [],
      "source": [
        "# Normalizar datos\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train = scaler_X.fit_transform(X_train)\n",
        "X_eval = scaler_X.transform(X_eval)\n",
        "X_test = scaler_X.transform(X_test)\n",
        "\n",
        "y_train = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
        "y_eval = scaler_y.transform(y_eval.values.reshape(-1, 1))\n",
        "y_test = scaler_y.transform(y_test.values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "B_kCHdWQaMJv"
      },
      "outputs": [],
      "source": [
        "# Convertir a tensores\n",
        "def to_tensor(X, y):\n",
        "    return torch.tensor(X, dtype=torch.float32, device=device), torch.tensor(y, dtype=torch.float32, device=device)\n",
        "\n",
        "X_train_tensor, y_train_tensor = to_tensor(X_train, y_train)\n",
        "X_eval_tensor, y_eval_tensor = to_tensor(X_eval, y_eval)\n",
        "X_test_tensor, y_test_tensor = to_tensor(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7gl35E6OaOSO"
      },
      "outputs": [],
      "source": [
        "# Cargar en DataLoader\n",
        "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True)\n",
        "eval_loader = DataLoader(TensorDataset(X_eval_tensor, y_eval_tensor), batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EN47kJlmaWZ8"
      },
      "outputs": [],
      "source": [
        "## Definir modelo LSTM 1\n",
        "#class StressLSTM(nn.Module):\n",
        "#    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "#        super(StressLSTM, self).__init__()\n",
        "#        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "#        self.fc = nn.Linear(hidden_dim, 1)\n",
        "#\n",
        "#    def forward(self, x):\n",
        "#        lstm_out, _ = self.lstm(x.unsqueeze(1))  # Agregar dimensi√≥n temporal\n",
        "#        return self.fc(lstm_out[:, -1, :])  # Tomar la √∫ltima salida del LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zrfjjQFidOjS"
      },
      "outputs": [],
      "source": [
        "class StressLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
        "        super(StressLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.batch_norm = nn.BatchNorm1d(hidden_dim)  # Batch Normalization\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x.unsqueeze(1))  # (batch, seq_len=1, input_dim)\n",
        "        lstm_out = self.batch_norm(lstm_out[:, -1, :])  # √öltima salida del LSTM con batch norm\n",
        "        return self.fc(lstm_out)  # Pasar por la capa totalmente conectada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CRJtyUoVaXU4"
      },
      "outputs": [],
      "source": [
        "## Optuna para optimizar hiperpar√°metros_1\n",
        "#def objective(trial):\n",
        "#    hidden_dim = trial.suggest_int(\"hidden_dim\", 16, 128)\n",
        "#    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
        "#    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"SGD\", \"Adam\", \"RMSprop\"])\n",
        "#    learning_rate = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
        "#    criterion_name = trial.suggest_categorical(\"criterion\", [\"MSELoss\", \"L1Loss\", \"HuberLoss\"])\n",
        "#\n",
        "#    model = StressLSTM(input_dim=X_train.shape[1], hidden_dim=hidden_dim, num_layers=num_layers).to(device)\n",
        "#    criterion = getattr(nn, criterion_name)()\n",
        "#    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=learning_rate)\n",
        "#\n",
        "#    # Entrenamiento\n",
        "#    for epoch in range(10):\n",
        "#        model.train()\n",
        "#        for X_batch, y_batch in train_loader:\n",
        "#            optimizer.zero_grad()\n",
        "#            y_pred = model(X_batch)\n",
        "#            loss = criterion(y_pred, y_batch)\n",
        "#            loss.backward()\n",
        "#            optimizer.step()\n",
        "#\n",
        "#    # Evaluaci√≥n en conjunto de validaci√≥n\n",
        "#    model.eval()\n",
        "#    eval_loss = 0\n",
        "#    with torch.no_grad():\n",
        "#        for X_batch, y_batch in eval_loader:\n",
        "#            y_pred = model(X_batch)\n",
        "#            eval_loss += criterion(y_pred, y_batch).item()\n",
        "#\n",
        "#    return eval_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snOd-zQGdXBp",
        "outputId": "be04207b-d8e2-4bd6-f25c-a69029c89ad8"
      },
      "outputs": [],
      "source": [
        "#def objective(trial):\n",
        "#    hidden_dim = trial.suggest_int(\"hidden_dim\", 16, 128)\n",
        "#    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
        "#    dropout = trial.suggest_float(\"dropout\", 0.2, 0.5) if num_layers > 1 else 0.0\n",
        "#    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"SGD\", \"Adam\", \"RMSprop\"])\n",
        "#    learning_rate = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
        "#    criterion_name = trial.suggest_categorical(\"criterion\", [\"MSELoss\", \"L1Loss\", \"HuberLoss\"])\n",
        "#\n",
        "#    # Inicializar modelo\n",
        "#    model = StressLSTM(input_dim=X_train.shape[1], hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout).to(device)\n",
        "#    criterion = getattr(nn, criterion_name)()\n",
        "#    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=learning_rate)\n",
        "#\n",
        "#    # Entrenamiento\n",
        "#    for epoch in range(10):\n",
        "#        model.train()\n",
        "#        for X_batch, y_batch in train_loader:\n",
        "#            optimizer.zero_grad()\n",
        "#            y_pred = model(X_batch)\n",
        "#            loss = criterion(y_pred, y_batch)\n",
        "#            loss.backward()\n",
        "#            optimizer.step()\n",
        "#\n",
        "#    # Evaluaci√≥n en conjunto de validaci√≥n\n",
        "#    model.eval()\n",
        "#    eval_loss = 0\n",
        "#    with torch.no_grad():\n",
        "#        for X_batch, y_batch in eval_loader:\n",
        "#            y_pred = model(X_batch)\n",
        "#            eval_loss += criterion(y_pred, y_batch).item()\n",
        "#\n",
        "#    return eval_loss\n",
        "#\n",
        "## Ejecutar Optuna\n",
        "#study = optuna.create_study(direction=\"minimize\")\n",
        "#study.optimize(objective, n_trials=50)\n",
        "#\n",
        "## Mejor conjunto de hiperpar√°metros\n",
        "#best_params = study.best_params\n",
        "#print(\"Mejores Hiperpar√°metros:\", best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fBgaaxFjgNzW"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    hidden_dim = trial.suggest_int(\"hidden_dim\", 16, 128)\n",
        "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5) if num_layers > 1 else 0.0\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"SGD\", \"Adam\", \"RMSprop\"])\n",
        "    learning_rate = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
        "    criterion_name = trial.suggest_categorical(\"criterion\", [\"MSELoss\", \"L1Loss\", \"HuberLoss\"])\n",
        "\n",
        "    model = StressLSTM(input_dim=X_train.shape[1], hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout).to(device)\n",
        "    criterion = getattr(nn, criterion_name)()\n",
        "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    patience = 5\n",
        "    best_loss = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(50):  # Aumentar n√∫mero de √©pocas si es necesario\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(X_batch)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        eval_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in eval_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                y_pred = model(X_batch)\n",
        "                eval_loss += criterion(y_pred, y_batch).item()\n",
        "        eval_loss /= len(eval_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f} | Eval Loss: {eval_loss:.4f}\")\n",
        "\n",
        "        if eval_loss < best_loss:\n",
        "            best_loss = eval_loss\n",
        "            best_model = model.state_dict()\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_model)\n",
        "    return best_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1hdZK9Zaaw8",
        "outputId": "8476724a-0335-4e38-df84-145478b21077"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:45:31,095] A new study created in memory with name: no-name-a9be8806-374e-465c-af71-a3edd5953ecd\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.3903 | Eval Loss: 0.3602\n",
            "Epoch 2: Train Loss: 0.2633 | Eval Loss: 0.2150\n",
            "Epoch 3: Train Loss: 0.1950 | Eval Loss: 0.1364\n",
            "Epoch 4: Train Loss: 0.1756 | Eval Loss: 0.1204\n",
            "Epoch 5: Train Loss: 0.1633 | Eval Loss: 0.0966\n",
            "Epoch 6: Train Loss: 0.1283 | Eval Loss: 0.1050\n",
            "Epoch 7: Train Loss: 0.1280 | Eval Loss: 0.0896\n",
            "Epoch 8: Train Loss: 0.1216 | Eval Loss: 0.0905\n",
            "Epoch 9: Train Loss: 0.1076 | Eval Loss: 0.0859\n",
            "Epoch 10: Train Loss: 0.1082 | Eval Loss: 0.0748\n",
            "Epoch 11: Train Loss: 0.0921 | Eval Loss: 0.0734\n",
            "Epoch 12: Train Loss: 0.0972 | Eval Loss: 0.0747\n",
            "Epoch 13: Train Loss: 0.0844 | Eval Loss: 0.0671\n",
            "Epoch 14: Train Loss: 0.0862 | Eval Loss: 0.0620\n",
            "Epoch 15: Train Loss: 0.0791 | Eval Loss: 0.0631\n",
            "Epoch 16: Train Loss: 0.0840 | Eval Loss: 0.0719\n",
            "Epoch 17: Train Loss: 0.0755 | Eval Loss: 0.0570\n",
            "Epoch 18: Train Loss: 0.0705 | Eval Loss: 0.0602\n",
            "Epoch 19: Train Loss: 0.0770 | Eval Loss: 0.0619\n",
            "Epoch 20: Train Loss: 0.0748 | Eval Loss: 0.0554\n",
            "Epoch 21: Train Loss: 0.0759 | Eval Loss: 0.0613\n",
            "Epoch 22: Train Loss: 0.0794 | Eval Loss: 0.0543\n",
            "Epoch 23: Train Loss: 0.0759 | Eval Loss: 0.0615\n",
            "Epoch 24: Train Loss: 0.0755 | Eval Loss: 0.0577\n",
            "Epoch 25: Train Loss: 0.0729 | Eval Loss: 0.0594\n",
            "Epoch 26: Train Loss: 0.0793 | Eval Loss: 0.0620\n",
            "Epoch 27: Train Loss: 0.0658 | Eval Loss: 0.0532\n",
            "Epoch 28: Train Loss: 0.0744 | Eval Loss: 0.0650\n",
            "Epoch 29: Train Loss: 0.0638 | Eval Loss: 0.0541\n",
            "Epoch 30: Train Loss: 0.0751 | Eval Loss: 0.0580\n",
            "Epoch 31: Train Loss: 0.0718 | Eval Loss: 0.0597\n",
            "Epoch 32: Train Loss: 0.0722 | Eval Loss: 0.0527\n",
            "Epoch 33: Train Loss: 0.0608 | Eval Loss: 0.0601\n",
            "Epoch 34: Train Loss: 0.0650 | Eval Loss: 0.0495\n",
            "Epoch 35: Train Loss: 0.0558 | Eval Loss: 0.0475\n",
            "Epoch 36: Train Loss: 0.0576 | Eval Loss: 0.0575\n",
            "Epoch 37: Train Loss: 0.0621 | Eval Loss: 0.0486\n",
            "Epoch 38: Train Loss: 0.0576 | Eval Loss: 0.0579\n",
            "Epoch 39: Train Loss: 0.0593 | Eval Loss: 0.0508\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:45:38,751] Trial 0 finished with value: 0.047509434906875384 and parameters: {'hidden_dim': 16, 'num_layers': 3, 'dropout': 0.3589018487965726, 'optimizer': 'Adam', 'lr': 0.009025866951602434, 'criterion': 'HuberLoss'}. Best is trial 0 with value: 0.047509434906875384.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40: Train Loss: 0.0629 | Eval Loss: 0.0576\n",
            "Early stopping triggered at epoch 40\n",
            "Epoch 1: Train Loss: 0.8515 | Eval Loss: 0.9547\n",
            "Epoch 2: Train Loss: 0.5499 | Eval Loss: 0.5028\n",
            "Epoch 3: Train Loss: 0.4361 | Eval Loss: 0.3617\n",
            "Epoch 4: Train Loss: 0.3691 | Eval Loss: 0.3234\n",
            "Epoch 5: Train Loss: 0.3151 | Eval Loss: 0.2922\n",
            "Epoch 6: Train Loss: 0.2876 | Eval Loss: 0.2588\n",
            "Epoch 7: Train Loss: 0.2459 | Eval Loss: 0.2350\n",
            "Epoch 8: Train Loss: 0.2206 | Eval Loss: 0.2240\n",
            "Epoch 9: Train Loss: 0.2113 | Eval Loss: 0.2128\n",
            "Epoch 10: Train Loss: 0.1898 | Eval Loss: 0.2113\n",
            "Epoch 11: Train Loss: 0.1936 | Eval Loss: 0.2124\n",
            "Epoch 12: Train Loss: 0.1766 | Eval Loss: 0.1989\n",
            "Epoch 13: Train Loss: 0.1554 | Eval Loss: 0.1839\n",
            "Epoch 14: Train Loss: 0.1604 | Eval Loss: 0.1835\n",
            "Epoch 15: Train Loss: 0.1523 | Eval Loss: 0.1706\n",
            "Epoch 16: Train Loss: 0.1568 | Eval Loss: 0.1878\n",
            "Epoch 17: Train Loss: 0.1408 | Eval Loss: 0.1664\n",
            "Epoch 18: Train Loss: 0.1350 | Eval Loss: 0.1631\n",
            "Epoch 19: Train Loss: 0.1332 | Eval Loss: 0.1782\n",
            "Epoch 20: Train Loss: 0.1249 | Eval Loss: 0.1648\n",
            "Epoch 21: Train Loss: 0.1281 | Eval Loss: 0.1560\n",
            "Epoch 22: Train Loss: 0.1154 | Eval Loss: 0.1571\n",
            "Epoch 23: Train Loss: 0.1079 | Eval Loss: 0.1641\n",
            "Epoch 24: Train Loss: 0.1137 | Eval Loss: 0.1550\n",
            "Epoch 25: Train Loss: 0.1200 | Eval Loss: 0.1522\n",
            "Epoch 26: Train Loss: 0.1105 | Eval Loss: 0.1504\n",
            "Epoch 27: Train Loss: 0.1120 | Eval Loss: 0.1496\n",
            "Epoch 28: Train Loss: 0.1000 | Eval Loss: 0.1465\n",
            "Epoch 29: Train Loss: 0.0944 | Eval Loss: 0.1453\n",
            "Epoch 30: Train Loss: 0.0928 | Eval Loss: 0.1508\n",
            "Epoch 31: Train Loss: 0.0921 | Eval Loss: 0.1380\n",
            "Epoch 32: Train Loss: 0.1025 | Eval Loss: 0.1367\n",
            "Epoch 33: Train Loss: 0.0895 | Eval Loss: 0.1526\n",
            "Epoch 34: Train Loss: 0.0893 | Eval Loss: 0.1400\n",
            "Epoch 35: Train Loss: 0.0951 | Eval Loss: 0.1422\n",
            "Epoch 36: Train Loss: 0.0953 | Eval Loss: 0.1285\n",
            "Epoch 37: Train Loss: 0.0995 | Eval Loss: 0.1397\n",
            "Epoch 38: Train Loss: 0.0990 | Eval Loss: 0.1291\n",
            "Epoch 39: Train Loss: 0.0862 | Eval Loss: 0.1426\n",
            "Epoch 40: Train Loss: 0.0882 | Eval Loss: 0.1347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:45:47,594] Trial 1 finished with value: 0.12851247191429138 and parameters: {'hidden_dim': 124, 'num_layers': 3, 'dropout': 0.29370266126815014, 'optimizer': 'Adam', 'lr': 0.0006813503834572497, 'criterion': 'MSELoss'}. Best is trial 0 with value: 0.047509434906875384.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41: Train Loss: 0.0865 | Eval Loss: 0.1424\n",
            "Early stopping triggered at epoch 41\n",
            "Epoch 1: Train Loss: 1.1080 | Eval Loss: 1.0065\n",
            "Epoch 2: Train Loss: 0.9506 | Eval Loss: 0.8928\n",
            "Epoch 3: Train Loss: 0.8373 | Eval Loss: 0.7983\n",
            "Epoch 4: Train Loss: 0.7971 | Eval Loss: 0.7188\n",
            "Epoch 5: Train Loss: 0.7129 | Eval Loss: 0.6442\n",
            "Epoch 6: Train Loss: 0.6534 | Eval Loss: 0.5733\n",
            "Epoch 7: Train Loss: 0.5604 | Eval Loss: 0.5220\n",
            "Epoch 8: Train Loss: 0.5419 | Eval Loss: 0.4892\n",
            "Epoch 9: Train Loss: 0.5083 | Eval Loss: 0.4401\n",
            "Epoch 10: Train Loss: 0.4600 | Eval Loss: 0.4163\n",
            "Epoch 11: Train Loss: 0.4251 | Eval Loss: 0.3787\n",
            "Epoch 12: Train Loss: 0.3985 | Eval Loss: 0.3648\n",
            "Epoch 13: Train Loss: 0.3793 | Eval Loss: 0.3539\n",
            "Epoch 14: Train Loss: 0.3621 | Eval Loss: 0.3415\n",
            "Epoch 15: Train Loss: 0.3554 | Eval Loss: 0.3260\n",
            "Epoch 16: Train Loss: 0.3287 | Eval Loss: 0.3043\n",
            "Epoch 17: Train Loss: 0.3167 | Eval Loss: 0.3071\n",
            "Epoch 18: Train Loss: 0.3155 | Eval Loss: 0.3031\n",
            "Epoch 19: Train Loss: 0.3089 | Eval Loss: 0.2873\n",
            "Epoch 20: Train Loss: 0.2993 | Eval Loss: 0.2799\n",
            "Epoch 21: Train Loss: 0.2783 | Eval Loss: 0.2715\n",
            "Epoch 22: Train Loss: 0.2893 | Eval Loss: 0.2713\n",
            "Epoch 23: Train Loss: 0.2665 | Eval Loss: 0.2807\n",
            "Epoch 24: Train Loss: 0.2526 | Eval Loss: 0.2570\n",
            "Epoch 25: Train Loss: 0.2674 | Eval Loss: 0.2622\n",
            "Epoch 26: Train Loss: 0.2707 | Eval Loss: 0.2630\n",
            "Epoch 27: Train Loss: 0.2505 | Eval Loss: 0.2472\n",
            "Epoch 28: Train Loss: 0.2539 | Eval Loss: 0.2588\n",
            "Epoch 29: Train Loss: 0.2284 | Eval Loss: 0.2336\n",
            "Epoch 30: Train Loss: 0.2245 | Eval Loss: 0.2318\n",
            "Epoch 31: Train Loss: 0.2346 | Eval Loss: 0.2346\n",
            "Epoch 32: Train Loss: 0.2565 | Eval Loss: 0.2439\n",
            "Epoch 33: Train Loss: 0.2017 | Eval Loss: 0.2221\n",
            "Epoch 34: Train Loss: 0.2024 | Eval Loss: 0.2313\n",
            "Epoch 35: Train Loss: 0.2056 | Eval Loss: 0.2211\n",
            "Epoch 36: Train Loss: 0.2159 | Eval Loss: 0.2123\n",
            "Epoch 37: Train Loss: 0.2108 | Eval Loss: 0.2140\n",
            "Epoch 38: Train Loss: 0.2141 | Eval Loss: 0.2199\n",
            "Epoch 39: Train Loss: 0.1972 | Eval Loss: 0.2163\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:45:56,022] Trial 2 finished with value: 0.21232914091909633 and parameters: {'hidden_dim': 36, 'num_layers': 3, 'dropout': 0.3220215287505668, 'optimizer': 'Adam', 'lr': 0.0002874314817128679, 'criterion': 'MSELoss'}. Best is trial 0 with value: 0.047509434906875384.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40: Train Loss: 0.2007 | Eval Loss: 0.2129\n",
            "Epoch 41: Train Loss: 0.2084 | Eval Loss: 0.2210\n",
            "Early stopping triggered at epoch 41\n",
            "Epoch 1: Train Loss: 0.9024 | Eval Loss: 0.8053\n",
            "Epoch 2: Train Loss: 0.7706 | Eval Loss: 0.7339\n",
            "Epoch 3: Train Loss: 0.7143 | Eval Loss: 0.6936\n",
            "Epoch 4: Train Loss: 0.6883 | Eval Loss: 0.6665\n",
            "Epoch 5: Train Loss: 0.6587 | Eval Loss: 0.6425\n",
            "Epoch 6: Train Loss: 0.6369 | Eval Loss: 0.6255\n",
            "Epoch 7: Train Loss: 0.6171 | Eval Loss: 0.6115\n",
            "Epoch 8: Train Loss: 0.6049 | Eval Loss: 0.6025\n",
            "Epoch 9: Train Loss: 0.5833 | Eval Loss: 0.5899\n",
            "Epoch 10: Train Loss: 0.5779 | Eval Loss: 0.5838\n",
            "Epoch 11: Train Loss: 0.5806 | Eval Loss: 0.5800\n",
            "Epoch 12: Train Loss: 0.5571 | Eval Loss: 0.5720\n",
            "Epoch 13: Train Loss: 0.5549 | Eval Loss: 0.5667\n",
            "Epoch 14: Train Loss: 0.5472 | Eval Loss: 0.5647\n",
            "Epoch 15: Train Loss: 0.5305 | Eval Loss: 0.5569\n",
            "Epoch 16: Train Loss: 0.5289 | Eval Loss: 0.5478\n",
            "Epoch 17: Train Loss: 0.5207 | Eval Loss: 0.5469\n",
            "Epoch 18: Train Loss: 0.5090 | Eval Loss: 0.5389\n",
            "Epoch 19: Train Loss: 0.4969 | Eval Loss: 0.5367\n",
            "Epoch 20: Train Loss: 0.4975 | Eval Loss: 0.5308\n",
            "Epoch 21: Train Loss: 0.4896 | Eval Loss: 0.5227\n",
            "Epoch 22: Train Loss: 0.4717 | Eval Loss: 0.5244\n",
            "Epoch 23: Train Loss: 0.4741 | Eval Loss: 0.5170\n",
            "Epoch 24: Train Loss: 0.4584 | Eval Loss: 0.5097\n",
            "Epoch 25: Train Loss: 0.4686 | Eval Loss: 0.5055\n",
            "Epoch 26: Train Loss: 0.4622 | Eval Loss: 0.5020\n",
            "Epoch 27: Train Loss: 0.4530 | Eval Loss: 0.4987\n",
            "Epoch 28: Train Loss: 0.4445 | Eval Loss: 0.4949\n",
            "Epoch 29: Train Loss: 0.4402 | Eval Loss: 0.4949\n",
            "Epoch 30: Train Loss: 0.4340 | Eval Loss: 0.4834\n",
            "Epoch 31: Train Loss: 0.4285 | Eval Loss: 0.4809\n",
            "Epoch 32: Train Loss: 0.4286 | Eval Loss: 0.4801\n",
            "Epoch 33: Train Loss: 0.4249 | Eval Loss: 0.4763\n",
            "Epoch 34: Train Loss: 0.4084 | Eval Loss: 0.4700\n",
            "Epoch 35: Train Loss: 0.4024 | Eval Loss: 0.4678\n",
            "Epoch 36: Train Loss: 0.4082 | Eval Loss: 0.4640\n",
            "Epoch 37: Train Loss: 0.3954 | Eval Loss: 0.4610\n",
            "Epoch 38: Train Loss: 0.3992 | Eval Loss: 0.4587\n",
            "Epoch 39: Train Loss: 0.3960 | Eval Loss: 0.4578\n",
            "Epoch 40: Train Loss: 0.3927 | Eval Loss: 0.4518\n",
            "Epoch 41: Train Loss: 0.3847 | Eval Loss: 0.4526\n",
            "Epoch 42: Train Loss: 0.3852 | Eval Loss: 0.4511\n",
            "Epoch 43: Train Loss: 0.3773 | Eval Loss: 0.4460\n",
            "Epoch 44: Train Loss: 0.3754 | Eval Loss: 0.4443\n",
            "Epoch 45: Train Loss: 0.3836 | Eval Loss: 0.4422\n",
            "Epoch 46: Train Loss: 0.3824 | Eval Loss: 0.4379\n",
            "Epoch 47: Train Loss: 0.3789 | Eval Loss: 0.4410\n",
            "Epoch 48: Train Loss: 0.3678 | Eval Loss: 0.4356\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:46:02,871] Trial 3 finished with value: 0.43193961241666007 and parameters: {'hidden_dim': 19, 'num_layers': 1, 'optimizer': 'SGD', 'lr': 0.007339169531779238, 'criterion': 'L1Loss'}. Best is trial 0 with value: 0.047509434906875384.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49: Train Loss: 0.3645 | Eval Loss: 0.4319\n",
            "Epoch 50: Train Loss: 0.3603 | Eval Loss: 0.4340\n",
            "Epoch 1: Train Loss: 0.9463 | Eval Loss: 0.8823\n",
            "Epoch 2: Train Loss: 0.8457 | Eval Loss: 0.8255\n",
            "Epoch 3: Train Loss: 0.7766 | Eval Loss: 0.7751\n",
            "Epoch 4: Train Loss: 0.7257 | Eval Loss: 0.7280\n",
            "Epoch 5: Train Loss: 0.6770 | Eval Loss: 0.6896\n",
            "Epoch 6: Train Loss: 0.6387 | Eval Loss: 0.6600\n",
            "Epoch 7: Train Loss: 0.5992 | Eval Loss: 0.6339\n",
            "Epoch 8: Train Loss: 0.5747 | Eval Loss: 0.6123\n",
            "Epoch 9: Train Loss: 0.5483 | Eval Loss: 0.5949\n",
            "Epoch 10: Train Loss: 0.5167 | Eval Loss: 0.5747\n",
            "Epoch 11: Train Loss: 0.5089 | Eval Loss: 0.5671\n",
            "Epoch 12: Train Loss: 0.4917 | Eval Loss: 0.5465\n",
            "Epoch 13: Train Loss: 0.4788 | Eval Loss: 0.5361\n",
            "Epoch 14: Train Loss: 0.4568 | Eval Loss: 0.5215\n",
            "Epoch 15: Train Loss: 0.4372 | Eval Loss: 0.5106\n",
            "Epoch 16: Train Loss: 0.4186 | Eval Loss: 0.4983\n",
            "Epoch 17: Train Loss: 0.4177 | Eval Loss: 0.4916\n",
            "Epoch 18: Train Loss: 0.4014 | Eval Loss: 0.4806\n",
            "Epoch 19: Train Loss: 0.3899 | Eval Loss: 0.4729\n",
            "Epoch 20: Train Loss: 0.3888 | Eval Loss: 0.4642\n",
            "Epoch 21: Train Loss: 0.3805 | Eval Loss: 0.4555\n",
            "Epoch 22: Train Loss: 0.3762 | Eval Loss: 0.4503\n",
            "Epoch 23: Train Loss: 0.3733 | Eval Loss: 0.4459\n",
            "Epoch 24: Train Loss: 0.3648 | Eval Loss: 0.4412\n",
            "Epoch 25: Train Loss: 0.3403 | Eval Loss: 0.4348\n",
            "Epoch 26: Train Loss: 0.3377 | Eval Loss: 0.4299\n",
            "Epoch 27: Train Loss: 0.3346 | Eval Loss: 0.4260\n",
            "Epoch 28: Train Loss: 0.3208 | Eval Loss: 0.4212\n",
            "Epoch 29: Train Loss: 0.3357 | Eval Loss: 0.4276\n",
            "Epoch 30: Train Loss: 0.3144 | Eval Loss: 0.4202\n",
            "Epoch 31: Train Loss: 0.3041 | Eval Loss: 0.4124\n",
            "Epoch 32: Train Loss: 0.3034 | Eval Loss: 0.4151\n",
            "Epoch 33: Train Loss: 0.3215 | Eval Loss: 0.4095\n",
            "Epoch 34: Train Loss: 0.3042 | Eval Loss: 0.4032\n",
            "Epoch 35: Train Loss: 0.2995 | Eval Loss: 0.4018\n",
            "Epoch 36: Train Loss: 0.2896 | Eval Loss: 0.3984\n",
            "Epoch 37: Train Loss: 0.2845 | Eval Loss: 0.3977\n",
            "Epoch 38: Train Loss: 0.2908 | Eval Loss: 0.3955\n",
            "Epoch 39: Train Loss: 0.2889 | Eval Loss: 0.3936\n",
            "Epoch 40: Train Loss: 0.2821 | Eval Loss: 0.3973\n",
            "Epoch 41: Train Loss: 0.2670 | Eval Loss: 0.3938\n",
            "Epoch 42: Train Loss: 0.2666 | Eval Loss: 0.3867\n",
            "Epoch 43: Train Loss: 0.2705 | Eval Loss: 0.3861\n",
            "Epoch 44: Train Loss: 0.2697 | Eval Loss: 0.3879\n",
            "Epoch 45: Train Loss: 0.2404 | Eval Loss: 0.3818\n",
            "Epoch 46: Train Loss: 0.2706 | Eval Loss: 0.3789\n",
            "Epoch 47: Train Loss: 0.2613 | Eval Loss: 0.3796\n",
            "Epoch 48: Train Loss: 0.2583 | Eval Loss: 0.3765\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:46:10,968] Trial 4 finished with value: 0.37653837835087495 and parameters: {'hidden_dim': 30, 'num_layers': 1, 'optimizer': 'Adam', 'lr': 0.00014197946057034372, 'criterion': 'L1Loss'}. Best is trial 0 with value: 0.047509434906875384.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49: Train Loss: 0.2649 | Eval Loss: 0.3794\n",
            "Epoch 50: Train Loss: 0.2445 | Eval Loss: 0.3816\n",
            "Epoch 1: Train Loss: 0.6472 | Eval Loss: 0.5462\n",
            "Epoch 2: Train Loss: 0.4841 | Eval Loss: 0.3923\n",
            "Epoch 3: Train Loss: 0.3797 | Eval Loss: 0.3814\n",
            "Epoch 4: Train Loss: 0.3457 | Eval Loss: 0.3385\n",
            "Epoch 5: Train Loss: 0.3226 | Eval Loss: 0.3190\n",
            "Epoch 6: Train Loss: 0.2952 | Eval Loss: 0.3196\n",
            "Epoch 7: Train Loss: 0.2799 | Eval Loss: 0.2999\n",
            "Epoch 8: Train Loss: 0.2783 | Eval Loss: 0.2700\n",
            "Epoch 9: Train Loss: 0.2799 | Eval Loss: 0.2479\n",
            "Epoch 10: Train Loss: 0.2621 | Eval Loss: 0.2496\n",
            "Epoch 11: Train Loss: 0.2665 | Eval Loss: 0.2469\n",
            "Epoch 12: Train Loss: 0.2637 | Eval Loss: 0.2747\n",
            "Epoch 13: Train Loss: 0.2392 | Eval Loss: 0.2724\n",
            "Epoch 14: Train Loss: 0.2329 | Eval Loss: 0.2354\n",
            "Epoch 15: Train Loss: 0.2132 | Eval Loss: 0.2602\n",
            "Epoch 16: Train Loss: 0.2446 | Eval Loss: 0.2269\n",
            "Epoch 17: Train Loss: 0.2383 | Eval Loss: 0.2157\n",
            "Epoch 18: Train Loss: 0.2298 | Eval Loss: 0.2093\n",
            "Epoch 19: Train Loss: 0.2450 | Eval Loss: 0.2405\n",
            "Epoch 20: Train Loss: 0.2306 | Eval Loss: 0.2100\n",
            "Epoch 21: Train Loss: 0.2156 | Eval Loss: 0.2175\n",
            "Epoch 22: Train Loss: 0.2308 | Eval Loss: 0.2209\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:46:14,847] Trial 5 finished with value: 0.2092614892651053 and parameters: {'hidden_dim': 20, 'num_layers': 2, 'dropout': 0.17514058470198948, 'optimizer': 'RMSprop', 'lr': 0.005611529442298361, 'criterion': 'L1Loss'}. Best is trial 0 with value: 0.047509434906875384.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Train Loss: 0.2228 | Eval Loss: 0.2281\n",
            "Early stopping triggered at epoch 23\n",
            "Epoch 1: Train Loss: 0.8138 | Eval Loss: 0.5710\n",
            "Epoch 2: Train Loss: 0.4386 | Eval Loss: 0.4271\n",
            "Epoch 3: Train Loss: 0.3054 | Eval Loss: 0.3686\n",
            "Epoch 4: Train Loss: 0.2392 | Eval Loss: 0.3310\n",
            "Epoch 5: Train Loss: 0.1926 | Eval Loss: 0.2977\n",
            "Epoch 6: Train Loss: 0.1734 | Eval Loss: 0.2939\n",
            "Epoch 7: Train Loss: 0.1324 | Eval Loss: 0.2716\n",
            "Epoch 8: Train Loss: 0.1258 | Eval Loss: 0.2757\n",
            "Epoch 9: Train Loss: 0.1222 | Eval Loss: 0.2459\n",
            "Epoch 10: Train Loss: 0.1119 | Eval Loss: 0.2665\n",
            "Epoch 11: Train Loss: 0.0911 | Eval Loss: 0.2432\n",
            "Epoch 12: Train Loss: 0.0985 | Eval Loss: 0.2695\n",
            "Epoch 13: Train Loss: 0.0880 | Eval Loss: 0.2430\n",
            "Epoch 14: Train Loss: 0.0907 | Eval Loss: 0.2371\n",
            "Epoch 15: Train Loss: 0.0793 | Eval Loss: 0.2161\n",
            "Epoch 16: Train Loss: 0.0596 | Eval Loss: 0.2304\n",
            "Epoch 17: Train Loss: 0.0686 | Eval Loss: 0.2229\n",
            "Epoch 18: Train Loss: 0.0651 | Eval Loss: 0.2188\n",
            "Epoch 19: Train Loss: 0.0712 | Eval Loss: 0.2184\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:46:18,297] Trial 6 finished with value: 0.2161082848906517 and parameters: {'hidden_dim': 105, 'num_layers': 1, 'optimizer': 'Adam', 'lr': 0.0003947755229344281, 'criterion': 'MSELoss'}. Best is trial 0 with value: 0.047509434906875384.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Train Loss: 0.0637 | Eval Loss: 0.2217\n",
            "Early stopping triggered at epoch 20\n",
            "Epoch 1: Train Loss: 0.6258 | Eval Loss: 0.5306\n",
            "Epoch 2: Train Loss: 0.4285 | Eval Loss: 0.4215\n",
            "Epoch 3: Train Loss: 0.3667 | Eval Loss: 0.4318\n",
            "Epoch 4: Train Loss: 0.3362 | Eval Loss: 0.4272\n",
            "Epoch 5: Train Loss: 0.3046 | Eval Loss: 0.3372\n",
            "Epoch 6: Train Loss: 0.2825 | Eval Loss: 0.3343\n",
            "Epoch 7: Train Loss: 0.2706 | Eval Loss: 0.3159\n",
            "Epoch 8: Train Loss: 0.2591 | Eval Loss: 0.3412\n",
            "Epoch 9: Train Loss: 0.2354 | Eval Loss: 0.3015\n",
            "Epoch 10: Train Loss: 0.2515 | Eval Loss: 0.3129\n",
            "Epoch 11: Train Loss: 0.2444 | Eval Loss: 0.2965\n",
            "Epoch 12: Train Loss: 0.2440 | Eval Loss: 0.3295\n",
            "Epoch 13: Train Loss: 0.2143 | Eval Loss: 0.3125\n",
            "Epoch 14: Train Loss: 0.2282 | Eval Loss: 0.2824\n",
            "Epoch 15: Train Loss: 0.2191 | Eval Loss: 0.2969\n",
            "Epoch 16: Train Loss: 0.2137 | Eval Loss: 0.2814\n",
            "Epoch 17: Train Loss: 0.2139 | Eval Loss: 0.2817\n",
            "Epoch 18: Train Loss: 0.1863 | Eval Loss: 0.2905\n",
            "Epoch 19: Train Loss: 0.2004 | Eval Loss: 0.3367\n",
            "Epoch 20: Train Loss: 0.1946 | Eval Loss: 0.2672\n",
            "Epoch 21: Train Loss: 0.2053 | Eval Loss: 0.2682\n",
            "Epoch 22: Train Loss: 0.1897 | Eval Loss: 0.2687\n",
            "Epoch 23: Train Loss: 0.2039 | Eval Loss: 0.2674\n",
            "Epoch 24: Train Loss: 0.2176 | Eval Loss: 0.2713\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:46:22,088] Trial 7 finished with value: 0.26716269026784334 and parameters: {'hidden_dim': 109, 'num_layers': 1, 'optimizer': 'RMSprop', 'lr': 0.0008263116519749416, 'criterion': 'L1Loss'}. Best is trial 0 with value: 0.047509434906875384.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Train Loss: 0.2027 | Eval Loss: 0.2998\n",
            "Early stopping triggered at epoch 25\n",
            "Epoch 1: Train Loss: 1.0779 | Eval Loss: 1.0154\n",
            "Epoch 2: Train Loss: 0.9900 | Eval Loss: 0.9510\n",
            "Epoch 3: Train Loss: 0.9364 | Eval Loss: 0.8969\n",
            "Epoch 4: Train Loss: 0.9352 | Eval Loss: 0.8789\n",
            "Epoch 5: Train Loss: 0.8996 | Eval Loss: 0.8346\n",
            "Epoch 6: Train Loss: 0.8728 | Eval Loss: 0.7916\n",
            "Epoch 7: Train Loss: 0.8048 | Eval Loss: 0.7269\n",
            "Epoch 8: Train Loss: 0.7675 | Eval Loss: 0.7093\n",
            "Epoch 9: Train Loss: 0.7540 | Eval Loss: 0.6688\n",
            "Epoch 10: Train Loss: 0.7163 | Eval Loss: 0.6440\n",
            "Epoch 11: Train Loss: 0.7046 | Eval Loss: 0.6265\n",
            "Epoch 12: Train Loss: 0.6886 | Eval Loss: 0.6101\n",
            "Epoch 13: Train Loss: 0.6438 | Eval Loss: 0.5935\n",
            "Epoch 14: Train Loss: 0.6033 | Eval Loss: 0.5562\n",
            "Epoch 15: Train Loss: 0.5859 | Eval Loss: 0.5436\n",
            "Epoch 16: Train Loss: 0.5896 | Eval Loss: 0.5222\n",
            "Epoch 17: Train Loss: 0.5573 | Eval Loss: 0.5106\n",
            "Epoch 18: Train Loss: 0.5652 | Eval Loss: 0.5076\n",
            "Epoch 19: Train Loss: 0.5566 | Eval Loss: 0.4855\n",
            "Epoch 20: Train Loss: 0.5289 | Eval Loss: 0.4851\n",
            "Epoch 21: Train Loss: 0.5394 | Eval Loss: 0.4667\n",
            "Epoch 22: Train Loss: 0.5102 | Eval Loss: 0.4588\n",
            "Epoch 23: Train Loss: 0.5095 | Eval Loss: 0.4406\n",
            "Epoch 24: Train Loss: 0.4785 | Eval Loss: 0.4504\n",
            "Epoch 25: Train Loss: 0.4739 | Eval Loss: 0.4381\n",
            "Epoch 26: Train Loss: 0.4765 | Eval Loss: 0.4105\n",
            "Epoch 27: Train Loss: 0.4635 | Eval Loss: 0.4106\n",
            "Epoch 28: Train Loss: 0.4584 | Eval Loss: 0.4084\n",
            "Epoch 29: Train Loss: 0.4603 | Eval Loss: 0.4286\n",
            "Epoch 30: Train Loss: 0.4367 | Eval Loss: 0.3917\n",
            "Epoch 31: Train Loss: 0.4420 | Eval Loss: 0.3979\n",
            "Epoch 32: Train Loss: 0.4134 | Eval Loss: 0.3916\n",
            "Epoch 33: Train Loss: 0.4188 | Eval Loss: 0.3874\n",
            "Epoch 34: Train Loss: 0.4233 | Eval Loss: 0.3763\n",
            "Epoch 35: Train Loss: 0.3856 | Eval Loss: 0.3886\n",
            "Epoch 36: Train Loss: 0.3896 | Eval Loss: 0.3832\n",
            "Epoch 37: Train Loss: 0.3910 | Eval Loss: 0.3563\n",
            "Epoch 38: Train Loss: 0.3936 | Eval Loss: 0.3671\n",
            "Epoch 39: Train Loss: 0.4143 | Eval Loss: 0.3674\n",
            "Epoch 40: Train Loss: 0.3969 | Eval Loss: 0.3559\n",
            "Epoch 41: Train Loss: 0.3710 | Eval Loss: 0.3422\n",
            "Epoch 42: Train Loss: 0.3632 | Eval Loss: 0.3496\n",
            "Epoch 43: Train Loss: 0.3753 | Eval Loss: 0.3468\n",
            "Epoch 44: Train Loss: 0.3626 | Eval Loss: 0.3328\n",
            "Epoch 45: Train Loss: 0.3758 | Eval Loss: 0.3476\n",
            "Epoch 46: Train Loss: 0.3588 | Eval Loss: 0.3265\n",
            "Epoch 47: Train Loss: 0.3349 | Eval Loss: 0.3241\n",
            "Epoch 48: Train Loss: 0.3543 | Eval Loss: 0.3287\n",
            "Epoch 49: Train Loss: 0.3570 | Eval Loss: 0.3207\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:46:31,247] Trial 8 finished with value: 0.31665456645628987 and parameters: {'hidden_dim': 39, 'num_layers': 3, 'dropout': 0.3378639593363911, 'optimizer': 'SGD', 'lr': 0.006426180313522199, 'criterion': 'MSELoss'}. Best is trial 0 with value: 0.047509434906875384.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50: Train Loss: 0.3306 | Eval Loss: 0.3167\n",
            "Epoch 1: Train Loss: 0.3644 | Eval Loss: 0.3586\n",
            "Epoch 2: Train Loss: 0.2565 | Eval Loss: 0.2172\n",
            "Epoch 3: Train Loss: 0.2072 | Eval Loss: 0.1767\n",
            "Epoch 4: Train Loss: 0.1662 | Eval Loss: 0.1442\n",
            "Epoch 5: Train Loss: 0.1381 | Eval Loss: 0.1403\n",
            "Epoch 6: Train Loss: 0.1297 | Eval Loss: 0.1292\n",
            "Epoch 7: Train Loss: 0.1048 | Eval Loss: 0.1225\n",
            "Epoch 8: Train Loss: 0.0973 | Eval Loss: 0.1215\n",
            "Epoch 9: Train Loss: 0.0849 | Eval Loss: 0.1101\n",
            "Epoch 10: Train Loss: 0.0815 | Eval Loss: 0.1112\n",
            "Epoch 11: Train Loss: 0.0709 | Eval Loss: 0.1078\n",
            "Epoch 12: Train Loss: 0.0770 | Eval Loss: 0.1005\n",
            "Epoch 13: Train Loss: 0.0755 | Eval Loss: 0.1068\n",
            "Epoch 14: Train Loss: 0.0717 | Eval Loss: 0.0965\n",
            "Epoch 15: Train Loss: 0.0668 | Eval Loss: 0.0942\n",
            "Epoch 16: Train Loss: 0.0638 | Eval Loss: 0.1000\n",
            "Epoch 17: Train Loss: 0.0622 | Eval Loss: 0.0944\n",
            "Epoch 18: Train Loss: 0.0571 | Eval Loss: 0.0949\n",
            "Epoch 19: Train Loss: 0.0608 | Eval Loss: 0.0907\n",
            "Epoch 20: Train Loss: 0.0599 | Eval Loss: 0.0892\n",
            "Epoch 21: Train Loss: 0.0606 | Eval Loss: 0.1003\n",
            "Epoch 22: Train Loss: 0.0532 | Eval Loss: 0.0925\n",
            "Epoch 23: Train Loss: 0.0520 | Eval Loss: 0.0912\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:46:35,324] Trial 9 finished with value: 0.08920150052975206 and parameters: {'hidden_dim': 35, 'num_layers': 2, 'dropout': 0.14635059045281854, 'optimizer': 'Adam', 'lr': 0.0007234563720977897, 'criterion': 'HuberLoss'}. Best is trial 0 with value: 0.047509434906875384.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: Train Loss: 0.0511 | Eval Loss: 0.0912\n",
            "Epoch 25: Train Loss: 0.0551 | Eval Loss: 0.0901\n",
            "Early stopping triggered at epoch 25\n",
            "Epoch 1: Train Loss: 0.4656 | Eval Loss: 0.4278\n",
            "Epoch 2: Train Loss: 0.4474 | Eval Loss: 0.4184\n",
            "Epoch 3: Train Loss: 0.4260 | Eval Loss: 0.4127\n",
            "Epoch 4: Train Loss: 0.4321 | Eval Loss: 0.4085\n",
            "Epoch 5: Train Loss: 0.4189 | Eval Loss: 0.4059\n",
            "Epoch 6: Train Loss: 0.4162 | Eval Loss: 0.4026\n",
            "Epoch 7: Train Loss: 0.4077 | Eval Loss: 0.4010\n",
            "Epoch 8: Train Loss: 0.4089 | Eval Loss: 0.3979\n",
            "Epoch 9: Train Loss: 0.4014 | Eval Loss: 0.3962\n",
            "Epoch 10: Train Loss: 0.4018 | Eval Loss: 0.3930\n",
            "Epoch 11: Train Loss: 0.4012 | Eval Loss: 0.3911\n",
            "Epoch 12: Train Loss: 0.3942 | Eval Loss: 0.3881\n",
            "Epoch 13: Train Loss: 0.3961 | Eval Loss: 0.3855\n",
            "Epoch 14: Train Loss: 0.3918 | Eval Loss: 0.3820\n",
            "Epoch 15: Train Loss: 0.3877 | Eval Loss: 0.3805\n",
            "Epoch 16: Train Loss: 0.3930 | Eval Loss: 0.3776\n",
            "Epoch 17: Train Loss: 0.3893 | Eval Loss: 0.3755\n",
            "Epoch 18: Train Loss: 0.3867 | Eval Loss: 0.3726\n",
            "Epoch 19: Train Loss: 0.3836 | Eval Loss: 0.3689\n",
            "Epoch 20: Train Loss: 0.3824 | Eval Loss: 0.3654\n",
            "Epoch 21: Train Loss: 0.3834 | Eval Loss: 0.3648\n",
            "Epoch 22: Train Loss: 0.3740 | Eval Loss: 0.3600\n",
            "Epoch 23: Train Loss: 0.3739 | Eval Loss: 0.3571\n",
            "Epoch 24: Train Loss: 0.3665 | Eval Loss: 0.3545\n",
            "Epoch 25: Train Loss: 0.3759 | Eval Loss: 0.3516\n",
            "Epoch 26: Train Loss: 0.3596 | Eval Loss: 0.3488\n",
            "Epoch 27: Train Loss: 0.3641 | Eval Loss: 0.3459\n",
            "Epoch 28: Train Loss: 0.3733 | Eval Loss: 0.3449\n",
            "Epoch 29: Train Loss: 0.3601 | Eval Loss: 0.3410\n",
            "Epoch 30: Train Loss: 0.3534 | Eval Loss: 0.3369\n",
            "Epoch 31: Train Loss: 0.3545 | Eval Loss: 0.3353\n",
            "Epoch 32: Train Loss: 0.3458 | Eval Loss: 0.3326\n",
            "Epoch 33: Train Loss: 0.3459 | Eval Loss: 0.3289\n",
            "Epoch 34: Train Loss: 0.3543 | Eval Loss: 0.3266\n",
            "Epoch 35: Train Loss: 0.3516 | Eval Loss: 0.3248\n",
            "Epoch 36: Train Loss: 0.3503 | Eval Loss: 0.3237\n",
            "Epoch 37: Train Loss: 0.3507 | Eval Loss: 0.3230\n",
            "Epoch 38: Train Loss: 0.3429 | Eval Loss: 0.3194\n",
            "Epoch 39: Train Loss: 0.3459 | Eval Loss: 0.3168\n",
            "Epoch 40: Train Loss: 0.3424 | Eval Loss: 0.3166\n",
            "Epoch 41: Train Loss: 0.3354 | Eval Loss: 0.3140\n",
            "Epoch 42: Train Loss: 0.3401 | Eval Loss: 0.3119\n",
            "Epoch 43: Train Loss: 0.3426 | Eval Loss: 0.3096\n",
            "Epoch 44: Train Loss: 0.3363 | Eval Loss: 0.3085\n",
            "Epoch 45: Train Loss: 0.3361 | Eval Loss: 0.3061\n",
            "Epoch 46: Train Loss: 0.3261 | Eval Loss: 0.3043\n",
            "Epoch 47: Train Loss: 0.3373 | Eval Loss: 0.3028\n",
            "Epoch 48: Train Loss: 0.3341 | Eval Loss: 0.3011\n",
            "Epoch 49: Train Loss: 0.3335 | Eval Loss: 0.3012\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:46:44,418] Trial 10 finished with value: 0.3011292438296711 and parameters: {'hidden_dim': 67, 'num_layers': 3, 'dropout': 0.4993055791257562, 'optimizer': 'SGD', 'lr': 0.0024985459571473276, 'criterion': 'HuberLoss'}. Best is trial 0 with value: 0.047509434906875384.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50: Train Loss: 0.3292 | Eval Loss: 0.3021\n",
            "Epoch 1: Train Loss: 0.3024 | Eval Loss: 0.2851\n",
            "Epoch 2: Train Loss: 0.1417 | Eval Loss: 0.1315\n",
            "Epoch 3: Train Loss: 0.0986 | Eval Loss: 0.1065\n",
            "Epoch 4: Train Loss: 0.0887 | Eval Loss: 0.0926\n",
            "Epoch 5: Train Loss: 0.0680 | Eval Loss: 0.0818\n",
            "Epoch 6: Train Loss: 0.0620 | Eval Loss: 0.0930\n",
            "Epoch 7: Train Loss: 0.0520 | Eval Loss: 0.0777\n",
            "Epoch 8: Train Loss: 0.0483 | Eval Loss: 0.0727\n",
            "Epoch 9: Train Loss: 0.0454 | Eval Loss: 0.0725\n",
            "Epoch 10: Train Loss: 0.0481 | Eval Loss: 0.0761\n",
            "Epoch 11: Train Loss: 0.0443 | Eval Loss: 0.0689\n",
            "Epoch 12: Train Loss: 0.0417 | Eval Loss: 0.0766\n",
            "Epoch 13: Train Loss: 0.0419 | Eval Loss: 0.0692\n",
            "Epoch 14: Train Loss: 0.0481 | Eval Loss: 0.0741\n",
            "Epoch 15: Train Loss: 0.0428 | Eval Loss: 0.0716\n",
            "Epoch 16: Train Loss: 0.0386 | Eval Loss: 0.0649\n",
            "Epoch 17: Train Loss: 0.0424 | Eval Loss: 0.0643\n",
            "Epoch 18: Train Loss: 0.0337 | Eval Loss: 0.0628\n",
            "Epoch 19: Train Loss: 0.0354 | Eval Loss: 0.0622\n",
            "Epoch 20: Train Loss: 0.0349 | Eval Loss: 0.0635\n",
            "Epoch 21: Train Loss: 0.0302 | Eval Loss: 0.0635\n",
            "Epoch 22: Train Loss: 0.0328 | Eval Loss: 0.0615\n",
            "Epoch 23: Train Loss: 0.0303 | Eval Loss: 0.0605\n",
            "Epoch 24: Train Loss: 0.0262 | Eval Loss: 0.0619\n",
            "Epoch 25: Train Loss: 0.0301 | Eval Loss: 0.0622\n",
            "Epoch 26: Train Loss: 0.0298 | Eval Loss: 0.0589\n",
            "Epoch 27: Train Loss: 0.0314 | Eval Loss: 0.0597\n",
            "Epoch 28: Train Loss: 0.0301 | Eval Loss: 0.0578\n",
            "Epoch 29: Train Loss: 0.0322 | Eval Loss: 0.0578\n",
            "Epoch 30: Train Loss: 0.0306 | Eval Loss: 0.0587\n",
            "Epoch 31: Train Loss: 0.0262 | Eval Loss: 0.0613\n",
            "Epoch 32: Train Loss: 0.0247 | Eval Loss: 0.0567\n",
            "Epoch 33: Train Loss: 0.0250 | Eval Loss: 0.0604\n",
            "Epoch 34: Train Loss: 0.0290 | Eval Loss: 0.0577\n",
            "Epoch 35: Train Loss: 0.0235 | Eval Loss: 0.0574\n",
            "Epoch 36: Train Loss: 0.0220 | Eval Loss: 0.0569\n",
            "Epoch 37: Train Loss: 0.0261 | Eval Loss: 0.0550\n",
            "Epoch 38: Train Loss: 0.0310 | Eval Loss: 0.0595\n",
            "Epoch 39: Train Loss: 0.0336 | Eval Loss: 0.0595\n",
            "Epoch 40: Train Loss: 0.0209 | Eval Loss: 0.0539\n",
            "Epoch 41: Train Loss: 0.0262 | Eval Loss: 0.0566\n",
            "Epoch 42: Train Loss: 0.0258 | Eval Loss: 0.0561\n",
            "Epoch 43: Train Loss: 0.0261 | Eval Loss: 0.0538\n",
            "Epoch 44: Train Loss: 0.0222 | Eval Loss: 0.0558\n",
            "Epoch 45: Train Loss: 0.0273 | Eval Loss: 0.0514\n",
            "Epoch 46: Train Loss: 0.0251 | Eval Loss: 0.0496\n",
            "Epoch 47: Train Loss: 0.0255 | Eval Loss: 0.0519\n",
            "Epoch 48: Train Loss: 0.0280 | Eval Loss: 0.0548\n",
            "Epoch 49: Train Loss: 0.0295 | Eval Loss: 0.0501\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:46:52,638] Trial 11 finished with value: 0.049630816039793635 and parameters: {'hidden_dim': 58, 'num_layers': 2, 'dropout': 0.11848625890081192, 'optimizer': 'Adam', 'lr': 0.0024231759422705186, 'criterion': 'HuberLoss'}. Best is trial 0 with value: 0.047509434906875384.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50: Train Loss: 0.0210 | Eval Loss: 0.0511\n",
            "Epoch 1: Train Loss: 0.3511 | Eval Loss: 0.3060\n",
            "Epoch 2: Train Loss: 0.2141 | Eval Loss: 0.1472\n",
            "Epoch 3: Train Loss: 0.1590 | Eval Loss: 0.1395\n",
            "Epoch 4: Train Loss: 0.1448 | Eval Loss: 0.1249\n",
            "Epoch 5: Train Loss: 0.1154 | Eval Loss: 0.1101\n",
            "Epoch 6: Train Loss: 0.1058 | Eval Loss: 0.0906\n",
            "Epoch 7: Train Loss: 0.0930 | Eval Loss: 0.0854\n",
            "Epoch 8: Train Loss: 0.0901 | Eval Loss: 0.0823\n",
            "Epoch 9: Train Loss: 0.0814 | Eval Loss: 0.0797\n",
            "Epoch 10: Train Loss: 0.0783 | Eval Loss: 0.0780\n",
            "Epoch 11: Train Loss: 0.0755 | Eval Loss: 0.0785\n",
            "Epoch 12: Train Loss: 0.0690 | Eval Loss: 0.0724\n",
            "Epoch 13: Train Loss: 0.0703 | Eval Loss: 0.0625\n",
            "Epoch 14: Train Loss: 0.0688 | Eval Loss: 0.0722\n",
            "Epoch 15: Train Loss: 0.0663 | Eval Loss: 0.0699\n",
            "Epoch 16: Train Loss: 0.0616 | Eval Loss: 0.0659\n",
            "Epoch 17: Train Loss: 0.0640 | Eval Loss: 0.0660\n",
            "Epoch 18: Train Loss: 0.0548 | Eval Loss: 0.0617\n",
            "Epoch 19: Train Loss: 0.0519 | Eval Loss: 0.0577\n",
            "Epoch 20: Train Loss: 0.0548 | Eval Loss: 0.0685\n",
            "Epoch 21: Train Loss: 0.0529 | Eval Loss: 0.0622\n",
            "Epoch 22: Train Loss: 0.0530 | Eval Loss: 0.0657\n",
            "Epoch 23: Train Loss: 0.0495 | Eval Loss: 0.0575\n",
            "Epoch 24: Train Loss: 0.0393 | Eval Loss: 0.0522\n",
            "Epoch 25: Train Loss: 0.0447 | Eval Loss: 0.0570\n",
            "Epoch 26: Train Loss: 0.0409 | Eval Loss: 0.0590\n",
            "Epoch 27: Train Loss: 0.0500 | Eval Loss: 0.0575\n",
            "Epoch 28: Train Loss: 0.0393 | Eval Loss: 0.0548\n",
            "Epoch 29: Train Loss: 0.0362 | Eval Loss: 0.0514\n",
            "Epoch 30: Train Loss: 0.0330 | Eval Loss: 0.0532\n",
            "Epoch 31: Train Loss: 0.0361 | Eval Loss: 0.0535\n",
            "Epoch 32: Train Loss: 0.0345 | Eval Loss: 0.0509\n",
            "Epoch 33: Train Loss: 0.0393 | Eval Loss: 0.0533\n",
            "Epoch 34: Train Loss: 0.0366 | Eval Loss: 0.0474\n",
            "Epoch 35: Train Loss: 0.0405 | Eval Loss: 0.0490\n",
            "Epoch 36: Train Loss: 0.0360 | Eval Loss: 0.0446\n",
            "Epoch 37: Train Loss: 0.0411 | Eval Loss: 0.0489\n",
            "Epoch 38: Train Loss: 0.0386 | Eval Loss: 0.0440\n",
            "Epoch 39: Train Loss: 0.0350 | Eval Loss: 0.0450\n",
            "Epoch 40: Train Loss: 0.0304 | Eval Loss: 0.0474\n",
            "Epoch 41: Train Loss: 0.0425 | Eval Loss: 0.0446\n",
            "Epoch 42: Train Loss: 0.0289 | Eval Loss: 0.0437\n",
            "Epoch 43: Train Loss: 0.0300 | Eval Loss: 0.0438\n",
            "Epoch 44: Train Loss: 0.0306 | Eval Loss: 0.0427\n",
            "Epoch 45: Train Loss: 0.0332 | Eval Loss: 0.0449\n",
            "Epoch 46: Train Loss: 0.0276 | Eval Loss: 0.0434\n",
            "Epoch 47: Train Loss: 0.0275 | Eval Loss: 0.0433\n",
            "Epoch 48: Train Loss: 0.0275 | Eval Loss: 0.0441\n",
            "Epoch 49: Train Loss: 0.0310 | Eval Loss: 0.0422\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:47:01,409] Trial 12 finished with value: 0.04215869417085367 and parameters: {'hidden_dim': 66, 'num_layers': 2, 'dropout': 0.43569622483440046, 'optimizer': 'Adam', 'lr': 0.002335789745233905, 'criterion': 'HuberLoss'}. Best is trial 12 with value: 0.04215869417085367.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50: Train Loss: 0.0279 | Eval Loss: 0.0432\n",
            "Epoch 1: Train Loss: 0.3535 | Eval Loss: 0.3037\n",
            "Epoch 2: Train Loss: 0.2208 | Eval Loss: 0.1494\n",
            "Epoch 3: Train Loss: 0.1545 | Eval Loss: 0.1305\n",
            "Epoch 4: Train Loss: 0.1282 | Eval Loss: 0.1095\n",
            "Epoch 5: Train Loss: 0.1097 | Eval Loss: 0.0994\n",
            "Epoch 6: Train Loss: 0.1006 | Eval Loss: 0.0939\n",
            "Epoch 7: Train Loss: 0.0936 | Eval Loss: 0.0875\n",
            "Epoch 8: Train Loss: 0.0907 | Eval Loss: 0.0812\n",
            "Epoch 9: Train Loss: 0.0822 | Eval Loss: 0.0738\n",
            "Epoch 10: Train Loss: 0.0782 | Eval Loss: 0.0673\n",
            "Epoch 11: Train Loss: 0.0723 | Eval Loss: 0.0735\n",
            "Epoch 12: Train Loss: 0.0661 | Eval Loss: 0.0682\n",
            "Epoch 13: Train Loss: 0.0552 | Eval Loss: 0.0624\n",
            "Epoch 14: Train Loss: 0.0690 | Eval Loss: 0.0599\n",
            "Epoch 15: Train Loss: 0.0618 | Eval Loss: 0.0675\n",
            "Epoch 16: Train Loss: 0.0619 | Eval Loss: 0.0661\n",
            "Epoch 17: Train Loss: 0.0511 | Eval Loss: 0.0590\n",
            "Epoch 18: Train Loss: 0.0513 | Eval Loss: 0.0609\n",
            "Epoch 19: Train Loss: 0.0542 | Eval Loss: 0.0652\n",
            "Epoch 20: Train Loss: 0.0512 | Eval Loss: 0.0600\n",
            "Epoch 21: Train Loss: 0.0493 | Eval Loss: 0.0605\n",
            "Epoch 22: Train Loss: 0.0482 | Eval Loss: 0.0551\n",
            "Epoch 23: Train Loss: 0.0500 | Eval Loss: 0.0599\n",
            "Epoch 24: Train Loss: 0.0436 | Eval Loss: 0.0501\n",
            "Epoch 25: Train Loss: 0.0437 | Eval Loss: 0.0483\n",
            "Epoch 26: Train Loss: 0.0368 | Eval Loss: 0.0494\n",
            "Epoch 27: Train Loss: 0.0387 | Eval Loss: 0.0541\n",
            "Epoch 28: Train Loss: 0.0383 | Eval Loss: 0.0502\n",
            "Epoch 29: Train Loss: 0.0393 | Eval Loss: 0.0477\n",
            "Epoch 30: Train Loss: 0.0397 | Eval Loss: 0.0465\n",
            "Epoch 31: Train Loss: 0.0380 | Eval Loss: 0.0494\n",
            "Epoch 32: Train Loss: 0.0336 | Eval Loss: 0.0448\n",
            "Epoch 33: Train Loss: 0.0372 | Eval Loss: 0.0444\n",
            "Epoch 34: Train Loss: 0.0365 | Eval Loss: 0.0435\n",
            "Epoch 35: Train Loss: 0.0321 | Eval Loss: 0.0414\n",
            "Epoch 36: Train Loss: 0.0298 | Eval Loss: 0.0414\n",
            "Epoch 37: Train Loss: 0.0285 | Eval Loss: 0.0430\n",
            "Epoch 38: Train Loss: 0.0330 | Eval Loss: 0.0395\n",
            "Epoch 39: Train Loss: 0.0305 | Eval Loss: 0.0393\n",
            "Epoch 40: Train Loss: 0.0283 | Eval Loss: 0.0406\n",
            "Epoch 41: Train Loss: 0.0320 | Eval Loss: 0.0405\n",
            "Epoch 42: Train Loss: 0.0334 | Eval Loss: 0.0403\n",
            "Epoch 43: Train Loss: 0.0282 | Eval Loss: 0.0371\n",
            "Epoch 44: Train Loss: 0.0312 | Eval Loss: 0.0378\n",
            "Epoch 45: Train Loss: 0.0299 | Eval Loss: 0.0357\n",
            "Epoch 46: Train Loss: 0.0268 | Eval Loss: 0.0391\n",
            "Epoch 47: Train Loss: 0.0266 | Eval Loss: 0.0411\n",
            "Epoch 48: Train Loss: 0.0329 | Eval Loss: 0.0337\n",
            "Epoch 49: Train Loss: 0.0237 | Eval Loss: 0.0336\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:47:10,293] Trial 13 finished with value: 0.033619585501797056 and parameters: {'hidden_dim': 88, 'num_layers': 2, 'dropout': 0.4550597814102049, 'optimizer': 'Adam', 'lr': 0.0024194873065368078, 'criterion': 'HuberLoss'}. Best is trial 13 with value: 0.033619585501797056.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50: Train Loss: 0.0243 | Eval Loss: 0.0350\n",
            "Epoch 1: Train Loss: 0.3604 | Eval Loss: 0.3091\n",
            "Epoch 2: Train Loss: 0.2327 | Eval Loss: 0.1728\n",
            "Epoch 3: Train Loss: 0.1810 | Eval Loss: 0.1397\n",
            "Epoch 4: Train Loss: 0.1450 | Eval Loss: 0.1223\n",
            "Epoch 5: Train Loss: 0.1305 | Eval Loss: 0.1118\n",
            "Epoch 6: Train Loss: 0.1302 | Eval Loss: 0.1098\n",
            "Epoch 7: Train Loss: 0.1068 | Eval Loss: 0.0986\n",
            "Epoch 8: Train Loss: 0.0913 | Eval Loss: 0.0871\n",
            "Epoch 9: Train Loss: 0.0858 | Eval Loss: 0.0816\n",
            "Epoch 10: Train Loss: 0.0820 | Eval Loss: 0.0770\n",
            "Epoch 11: Train Loss: 0.0777 | Eval Loss: 0.0797\n",
            "Epoch 12: Train Loss: 0.0730 | Eval Loss: 0.0765\n",
            "Epoch 13: Train Loss: 0.0760 | Eval Loss: 0.0705\n",
            "Epoch 14: Train Loss: 0.0633 | Eval Loss: 0.0692\n",
            "Epoch 15: Train Loss: 0.0606 | Eval Loss: 0.0640\n",
            "Epoch 16: Train Loss: 0.0631 | Eval Loss: 0.0630\n",
            "Epoch 17: Train Loss: 0.0659 | Eval Loss: 0.0630\n",
            "Epoch 18: Train Loss: 0.0564 | Eval Loss: 0.0649\n",
            "Epoch 19: Train Loss: 0.0601 | Eval Loss: 0.0602\n",
            "Epoch 20: Train Loss: 0.0562 | Eval Loss: 0.0646\n",
            "Epoch 21: Train Loss: 0.0592 | Eval Loss: 0.0625\n",
            "Epoch 22: Train Loss: 0.0550 | Eval Loss: 0.0570\n",
            "Epoch 23: Train Loss: 0.0553 | Eval Loss: 0.0594\n",
            "Epoch 24: Train Loss: 0.0482 | Eval Loss: 0.0574\n",
            "Epoch 25: Train Loss: 0.0469 | Eval Loss: 0.0585\n",
            "Epoch 26: Train Loss: 0.0457 | Eval Loss: 0.0520\n",
            "Epoch 27: Train Loss: 0.0425 | Eval Loss: 0.0529\n",
            "Epoch 28: Train Loss: 0.0380 | Eval Loss: 0.0529\n",
            "Epoch 29: Train Loss: 0.0405 | Eval Loss: 0.0499\n",
            "Epoch 30: Train Loss: 0.0439 | Eval Loss: 0.0493\n",
            "Epoch 31: Train Loss: 0.0353 | Eval Loss: 0.0485\n",
            "Epoch 32: Train Loss: 0.0427 | Eval Loss: 0.0505\n",
            "Epoch 33: Train Loss: 0.0373 | Eval Loss: 0.0521\n",
            "Epoch 34: Train Loss: 0.0382 | Eval Loss: 0.0511\n",
            "Epoch 35: Train Loss: 0.0394 | Eval Loss: 0.0451\n",
            "Epoch 36: Train Loss: 0.0414 | Eval Loss: 0.0471\n",
            "Epoch 37: Train Loss: 0.0365 | Eval Loss: 0.0445\n",
            "Epoch 38: Train Loss: 0.0340 | Eval Loss: 0.0438\n",
            "Epoch 39: Train Loss: 0.0355 | Eval Loss: 0.0418\n",
            "Epoch 40: Train Loss: 0.0379 | Eval Loss: 0.0456\n",
            "Epoch 41: Train Loss: 0.0351 | Eval Loss: 0.0386\n",
            "Epoch 42: Train Loss: 0.0293 | Eval Loss: 0.0377\n",
            "Epoch 43: Train Loss: 0.0296 | Eval Loss: 0.0372\n",
            "Epoch 44: Train Loss: 0.0337 | Eval Loss: 0.0376\n",
            "Epoch 45: Train Loss: 0.0332 | Eval Loss: 0.0415\n",
            "Epoch 46: Train Loss: 0.0275 | Eval Loss: 0.0345\n",
            "Epoch 47: Train Loss: 0.0286 | Eval Loss: 0.0368\n",
            "Epoch 48: Train Loss: 0.0302 | Eval Loss: 0.0367\n",
            "Epoch 49: Train Loss: 0.0302 | Eval Loss: 0.0360\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:47:19,296] Trial 14 finished with value: 0.03452780793475754 and parameters: {'hidden_dim': 88, 'num_layers': 2, 'dropout': 0.48513496863559186, 'optimizer': 'Adam', 'lr': 0.0021654398365495704, 'criterion': 'HuberLoss'}. Best is trial 13 with value: 0.033619585501797056.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50: Train Loss: 0.0311 | Eval Loss: 0.0362\n",
            "Epoch 1: Train Loss: 0.3511 | Eval Loss: 0.2512\n",
            "Epoch 2: Train Loss: 0.1989 | Eval Loss: 0.1533\n",
            "Epoch 3: Train Loss: 0.1603 | Eval Loss: 0.1323\n",
            "Epoch 4: Train Loss: 0.1447 | Eval Loss: 0.1228\n",
            "Epoch 5: Train Loss: 0.1299 | Eval Loss: 0.0992\n",
            "Epoch 6: Train Loss: 0.1116 | Eval Loss: 0.0927\n",
            "Epoch 7: Train Loss: 0.1047 | Eval Loss: 0.1007\n",
            "Epoch 8: Train Loss: 0.0908 | Eval Loss: 0.0812\n",
            "Epoch 9: Train Loss: 0.0911 | Eval Loss: 0.0885\n",
            "Epoch 10: Train Loss: 0.0822 | Eval Loss: 0.0699\n",
            "Epoch 11: Train Loss: 0.0770 | Eval Loss: 0.0807\n",
            "Epoch 12: Train Loss: 0.0813 | Eval Loss: 0.0661\n",
            "Epoch 13: Train Loss: 0.0689 | Eval Loss: 0.0711\n",
            "Epoch 14: Train Loss: 0.0728 | Eval Loss: 0.0651\n",
            "Epoch 15: Train Loss: 0.0692 | Eval Loss: 0.0723\n",
            "Epoch 16: Train Loss: 0.0672 | Eval Loss: 0.0673\n",
            "Epoch 17: Train Loss: 0.0616 | Eval Loss: 0.0669\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:47:22,393] Trial 15 finished with value: 0.06505721566431663 and parameters: {'hidden_dim': 83, 'num_layers': 2, 'dropout': 0.49827469582706263, 'optimizer': 'RMSprop', 'lr': 0.001498391521630183, 'criterion': 'HuberLoss'}. Best is trial 13 with value: 0.033619585501797056.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Train Loss: 0.0561 | Eval Loss: 0.0726\n",
            "Epoch 19: Train Loss: 0.0549 | Eval Loss: 0.0651\n",
            "Early stopping triggered at epoch 19\n",
            "Epoch 1: Train Loss: 0.3412 | Eval Loss: 0.2602\n",
            "Epoch 2: Train Loss: 0.1856 | Eval Loss: 0.1485\n",
            "Epoch 3: Train Loss: 0.1491 | Eval Loss: 0.1144\n",
            "Epoch 4: Train Loss: 0.1208 | Eval Loss: 0.0999\n",
            "Epoch 5: Train Loss: 0.0990 | Eval Loss: 0.0882\n",
            "Epoch 6: Train Loss: 0.0886 | Eval Loss: 0.0780\n",
            "Epoch 7: Train Loss: 0.0782 | Eval Loss: 0.0815\n",
            "Epoch 8: Train Loss: 0.0728 | Eval Loss: 0.0778\n",
            "Epoch 9: Train Loss: 0.0689 | Eval Loss: 0.0711\n",
            "Epoch 10: Train Loss: 0.0652 | Eval Loss: 0.0661\n",
            "Epoch 11: Train Loss: 0.0580 | Eval Loss: 0.0694\n",
            "Epoch 12: Train Loss: 0.0633 | Eval Loss: 0.0701\n",
            "Epoch 13: Train Loss: 0.0548 | Eval Loss: 0.0611\n",
            "Epoch 14: Train Loss: 0.0503 | Eval Loss: 0.0638\n",
            "Epoch 15: Train Loss: 0.0548 | Eval Loss: 0.0597\n",
            "Epoch 16: Train Loss: 0.0489 | Eval Loss: 0.0619\n",
            "Epoch 17: Train Loss: 0.0428 | Eval Loss: 0.0526\n",
            "Epoch 18: Train Loss: 0.0486 | Eval Loss: 0.0548\n",
            "Epoch 19: Train Loss: 0.0448 | Eval Loss: 0.0534\n",
            "Epoch 20: Train Loss: 0.0424 | Eval Loss: 0.0541\n",
            "Epoch 21: Train Loss: 0.0416 | Eval Loss: 0.0513\n",
            "Epoch 22: Train Loss: 0.0397 | Eval Loss: 0.0566\n",
            "Epoch 23: Train Loss: 0.0445 | Eval Loss: 0.0514\n",
            "Epoch 24: Train Loss: 0.0444 | Eval Loss: 0.0483\n",
            "Epoch 25: Train Loss: 0.0348 | Eval Loss: 0.0500\n",
            "Epoch 26: Train Loss: 0.0401 | Eval Loss: 0.0518\n",
            "Epoch 27: Train Loss: 0.0350 | Eval Loss: 0.0469\n",
            "Epoch 28: Train Loss: 0.0363 | Eval Loss: 0.0457\n",
            "Epoch 29: Train Loss: 0.0252 | Eval Loss: 0.0457\n",
            "Epoch 30: Train Loss: 0.0350 | Eval Loss: 0.0456\n",
            "Epoch 31: Train Loss: 0.0350 | Eval Loss: 0.0461\n",
            "Epoch 32: Train Loss: 0.0304 | Eval Loss: 0.0456\n",
            "Epoch 33: Train Loss: 0.0298 | Eval Loss: 0.0426\n",
            "Epoch 34: Train Loss: 0.0282 | Eval Loss: 0.0458\n",
            "Epoch 35: Train Loss: 0.0340 | Eval Loss: 0.0438\n",
            "Epoch 36: Train Loss: 0.0285 | Eval Loss: 0.0442\n",
            "Epoch 37: Train Loss: 0.0360 | Eval Loss: 0.0405\n",
            "Epoch 38: Train Loss: 0.0300 | Eval Loss: 0.0433\n",
            "Epoch 39: Train Loss: 0.0260 | Eval Loss: 0.0411\n",
            "Epoch 40: Train Loss: 0.0257 | Eval Loss: 0.0397\n",
            "Epoch 41: Train Loss: 0.0246 | Eval Loss: 0.0393\n",
            "Epoch 42: Train Loss: 0.0300 | Eval Loss: 0.0415\n",
            "Epoch 43: Train Loss: 0.0342 | Eval Loss: 0.0445\n",
            "Epoch 44: Train Loss: 0.0317 | Eval Loss: 0.0415\n",
            "Epoch 45: Train Loss: 0.0269 | Eval Loss: 0.0410\n",
            "Epoch 46: Train Loss: 0.0236 | Eval Loss: 0.0390\n",
            "Epoch 47: Train Loss: 0.0257 | Eval Loss: 0.0386\n",
            "Epoch 48: Train Loss: 0.0249 | Eval Loss: 0.0384\n",
            "Epoch 49: Train Loss: 0.0248 | Eval Loss: 0.0362\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:47:31,519] Trial 16 finished with value: 0.036222255953094536 and parameters: {'hidden_dim': 87, 'num_layers': 2, 'dropout': 0.41921394035836795, 'optimizer': 'Adam', 'lr': 0.003152305311323993, 'criterion': 'HuberLoss'}. Best is trial 13 with value: 0.033619585501797056.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50: Train Loss: 0.0235 | Eval Loss: 0.0369\n",
            "Epoch 1: Train Loss: 0.3377 | Eval Loss: 0.3175\n",
            "Epoch 2: Train Loss: 0.2256 | Eval Loss: 0.1780\n",
            "Epoch 3: Train Loss: 0.1912 | Eval Loss: 0.1489\n",
            "Epoch 4: Train Loss: 0.1518 | Eval Loss: 0.1325\n",
            "Epoch 5: Train Loss: 0.1294 | Eval Loss: 0.1155\n",
            "Epoch 6: Train Loss: 0.1113 | Eval Loss: 0.1161\n",
            "Epoch 7: Train Loss: 0.1070 | Eval Loss: 0.0937\n",
            "Epoch 8: Train Loss: 0.0910 | Eval Loss: 0.0921\n",
            "Epoch 9: Train Loss: 0.0893 | Eval Loss: 0.0890\n",
            "Epoch 10: Train Loss: 0.0806 | Eval Loss: 0.0838\n",
            "Epoch 11: Train Loss: 0.0837 | Eval Loss: 0.0900\n",
            "Epoch 12: Train Loss: 0.0762 | Eval Loss: 0.0739\n",
            "Epoch 13: Train Loss: 0.0772 | Eval Loss: 0.0736\n",
            "Epoch 14: Train Loss: 0.0633 | Eval Loss: 0.0766\n",
            "Epoch 15: Train Loss: 0.0703 | Eval Loss: 0.0762\n",
            "Epoch 16: Train Loss: 0.0645 | Eval Loss: 0.0719\n",
            "Epoch 17: Train Loss: 0.0577 | Eval Loss: 0.0695\n",
            "Epoch 18: Train Loss: 0.0608 | Eval Loss: 0.0663\n",
            "Epoch 19: Train Loss: 0.0660 | Eval Loss: 0.0644\n",
            "Epoch 20: Train Loss: 0.0533 | Eval Loss: 0.0668\n",
            "Epoch 21: Train Loss: 0.0564 | Eval Loss: 0.0621\n",
            "Epoch 22: Train Loss: 0.0542 | Eval Loss: 0.0656\n",
            "Epoch 23: Train Loss: 0.0600 | Eval Loss: 0.0684\n",
            "Epoch 24: Train Loss: 0.0536 | Eval Loss: 0.0636\n",
            "Epoch 25: Train Loss: 0.0522 | Eval Loss: 0.0605\n",
            "Epoch 26: Train Loss: 0.0482 | Eval Loss: 0.0579\n",
            "Epoch 27: Train Loss: 0.0518 | Eval Loss: 0.0626\n",
            "Epoch 28: Train Loss: 0.0439 | Eval Loss: 0.0608\n",
            "Epoch 29: Train Loss: 0.0490 | Eval Loss: 0.0629\n",
            "Epoch 30: Train Loss: 0.0515 | Eval Loss: 0.0571\n",
            "Epoch 31: Train Loss: 0.0502 | Eval Loss: 0.0572\n",
            "Epoch 32: Train Loss: 0.0428 | Eval Loss: 0.0537\n",
            "Epoch 33: Train Loss: 0.0421 | Eval Loss: 0.0554\n",
            "Epoch 34: Train Loss: 0.0440 | Eval Loss: 0.0521\n",
            "Epoch 35: Train Loss: 0.0439 | Eval Loss: 0.0562\n",
            "Epoch 36: Train Loss: 0.0453 | Eval Loss: 0.0546\n",
            "Epoch 37: Train Loss: 0.0373 | Eval Loss: 0.0554\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:47:38,190] Trial 17 finished with value: 0.052113702739862836 and parameters: {'hidden_dim': 92, 'num_layers': 2, 'dropout': 0.4197905250616065, 'optimizer': 'Adam', 'lr': 0.0013559498770464686, 'criterion': 'HuberLoss'}. Best is trial 13 with value: 0.033619585501797056.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38: Train Loss: 0.0406 | Eval Loss: 0.0541\n",
            "Epoch 39: Train Loss: 0.0390 | Eval Loss: 0.0525\n",
            "Early stopping triggered at epoch 39\n",
            "Epoch 1: Train Loss: 0.5143 | Eval Loss: 0.4507\n",
            "Epoch 2: Train Loss: 0.4398 | Eval Loss: 0.4103\n",
            "Epoch 3: Train Loss: 0.3959 | Eval Loss: 0.3800\n",
            "Epoch 4: Train Loss: 0.3728 | Eval Loss: 0.3623\n",
            "Epoch 5: Train Loss: 0.3544 | Eval Loss: 0.3467\n",
            "Epoch 6: Train Loss: 0.3383 | Eval Loss: 0.3357\n",
            "Epoch 7: Train Loss: 0.3310 | Eval Loss: 0.3286\n",
            "Epoch 8: Train Loss: 0.3184 | Eval Loss: 0.3204\n",
            "Epoch 9: Train Loss: 0.3119 | Eval Loss: 0.3139\n",
            "Epoch 10: Train Loss: 0.3049 | Eval Loss: 0.3050\n",
            "Epoch 11: Train Loss: 0.2965 | Eval Loss: 0.3011\n",
            "Epoch 12: Train Loss: 0.2913 | Eval Loss: 0.2961\n",
            "Epoch 13: Train Loss: 0.2842 | Eval Loss: 0.2920\n",
            "Epoch 14: Train Loss: 0.2769 | Eval Loss: 0.2857\n",
            "Epoch 15: Train Loss: 0.2711 | Eval Loss: 0.2796\n",
            "Epoch 16: Train Loss: 0.2613 | Eval Loss: 0.2775\n",
            "Epoch 17: Train Loss: 0.2572 | Eval Loss: 0.2716\n",
            "Epoch 18: Train Loss: 0.2561 | Eval Loss: 0.2667\n",
            "Epoch 19: Train Loss: 0.2442 | Eval Loss: 0.2622\n",
            "Epoch 20: Train Loss: 0.2425 | Eval Loss: 0.2576\n",
            "Epoch 21: Train Loss: 0.2335 | Eval Loss: 0.2535\n",
            "Epoch 22: Train Loss: 0.2360 | Eval Loss: 0.2516\n",
            "Epoch 23: Train Loss: 0.2243 | Eval Loss: 0.2454\n",
            "Epoch 24: Train Loss: 0.2188 | Eval Loss: 0.2419\n",
            "Epoch 25: Train Loss: 0.2087 | Eval Loss: 0.2393\n",
            "Epoch 26: Train Loss: 0.2109 | Eval Loss: 0.2361\n",
            "Epoch 27: Train Loss: 0.2115 | Eval Loss: 0.2317\n",
            "Epoch 28: Train Loss: 0.1988 | Eval Loss: 0.2318\n",
            "Epoch 29: Train Loss: 0.1921 | Eval Loss: 0.2272\n",
            "Epoch 30: Train Loss: 0.1844 | Eval Loss: 0.2241\n",
            "Epoch 31: Train Loss: 0.1817 | Eval Loss: 0.2187\n",
            "Epoch 32: Train Loss: 0.1746 | Eval Loss: 0.2172\n",
            "Epoch 33: Train Loss: 0.1762 | Eval Loss: 0.2148\n",
            "Epoch 34: Train Loss: 0.1752 | Eval Loss: 0.2133\n",
            "Epoch 35: Train Loss: 0.1696 | Eval Loss: 0.2106\n",
            "Epoch 36: Train Loss: 0.1643 | Eval Loss: 0.2073\n",
            "Epoch 37: Train Loss: 0.1647 | Eval Loss: 0.2059\n",
            "Epoch 38: Train Loss: 0.1536 | Eval Loss: 0.2033\n",
            "Epoch 39: Train Loss: 0.1569 | Eval Loss: 0.2008\n",
            "Epoch 40: Train Loss: 0.1521 | Eval Loss: 0.1978\n",
            "Epoch 41: Train Loss: 0.1456 | Eval Loss: 0.1970\n",
            "Epoch 42: Train Loss: 0.1520 | Eval Loss: 0.1948\n",
            "Epoch 43: Train Loss: 0.1376 | Eval Loss: 0.1930\n",
            "Epoch 44: Train Loss: 0.1365 | Eval Loss: 0.1900\n",
            "Epoch 45: Train Loss: 0.1347 | Eval Loss: 0.1892\n",
            "Epoch 46: Train Loss: 0.1327 | Eval Loss: 0.1848\n",
            "Epoch 47: Train Loss: 0.1334 | Eval Loss: 0.1849\n",
            "Epoch 48: Train Loss: 0.1208 | Eval Loss: 0.1827\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:47:45,578] Trial 18 finished with value: 0.17834423701552785 and parameters: {'hidden_dim': 51, 'num_layers': 1, 'optimizer': 'SGD', 'lr': 0.004218959663005482, 'criterion': 'HuberLoss'}. Best is trial 13 with value: 0.033619585501797056.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49: Train Loss: 0.1264 | Eval Loss: 0.1806\n",
            "Epoch 50: Train Loss: 0.1237 | Eval Loss: 0.1783\n",
            "Epoch 1: Train Loss: 0.3188 | Eval Loss: 0.2934\n",
            "Epoch 2: Train Loss: 0.1491 | Eval Loss: 0.1424\n",
            "Epoch 3: Train Loss: 0.1160 | Eval Loss: 0.1141\n",
            "Epoch 4: Train Loss: 0.0943 | Eval Loss: 0.1164\n",
            "Epoch 5: Train Loss: 0.0906 | Eval Loss: 0.0828\n",
            "Epoch 6: Train Loss: 0.0761 | Eval Loss: 0.0823\n",
            "Epoch 7: Train Loss: 0.0728 | Eval Loss: 0.0873\n",
            "Epoch 8: Train Loss: 0.0653 | Eval Loss: 0.0896\n",
            "Epoch 9: Train Loss: 0.0622 | Eval Loss: 0.0713\n",
            "Epoch 10: Train Loss: 0.0643 | Eval Loss: 0.0916\n",
            "Epoch 11: Train Loss: 0.0575 | Eval Loss: 0.0874\n",
            "Epoch 12: Train Loss: 0.0531 | Eval Loss: 0.0781\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-03-19 17:47:47,843] Trial 19 finished with value: 0.07126585043528501 and parameters: {'hidden_dim': 78, 'num_layers': 2, 'dropout': 0.25480650476838584, 'optimizer': 'RMSprop', 'lr': 0.0014187289363160412, 'criterion': 'HuberLoss'}. Best is trial 13 with value: 0.033619585501797056.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Train Loss: 0.0514 | Eval Loss: 0.0761\n",
            "Epoch 14: Train Loss: 0.0511 | Eval Loss: 0.0776\n",
            "Early stopping triggered at epoch 14\n",
            "Best trial: {'hidden_dim': 88, 'num_layers': 2, 'dropout': 0.4550597814102049, 'optimizer': 'Adam', 'lr': 0.0024194873065368078, 'criterion': 'HuberLoss'}\n"
          ]
        }
      ],
      "source": [
        "# Ejecutar Optuna\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "# Mostrar mejores par√°metros encontrados\n",
        "print(\"Best trial:\", study.best_trial.params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "16N6jhOrae39"
      },
      "outputs": [],
      "source": [
        "# Entrenar el mejor modelo\n",
        "best_params = study.best_trial.params\n",
        "model = StressLSTM(\n",
        "    input_dim=X_train.shape[1],\n",
        "    hidden_dim=best_params[\"hidden_dim\"],\n",
        "    num_layers=best_params[\"num_layers\"],\n",
        "    dropout=best_params[\"dropout\"]  # Agregar dropout\n",
        ").to(device)\n",
        "criterion = getattr(nn, best_params[\"criterion\"])()\n",
        "optimizer = getattr(optim, best_params[\"optimizer\"])(model.parameters(), lr=best_params[\"lr\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--Codigo Pablo, cargo mejores par√°metros para no volver a correr optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar el mejor modelo\n",
        "# Cargo los mejores par√°metros encontrados manualmente\n",
        "best_params = {'hidden_dim': 88, 'num_layers': 2, 'dropout': 0.4550597814102049, 'optimizer': 'Adam', 'lr': 0.0024194873065368078, 'criterion': 'HuberLoss'}  # Mejores hiperpar√°metros (excel)\n",
        "#best_params = study.best_trial.params\n",
        "model = StressLSTM(\n",
        "    input_dim=X_train.shape[1],\n",
        "    hidden_dim=best_params[\"hidden_dim\"],\n",
        "    num_layers=best_params[\"num_layers\"],\n",
        "    dropout=best_params[\"dropout\"]  # Agregar dropout\n",
        ").to(device)\n",
        "criterion = getattr(nn, best_params[\"criterion\"])()\n",
        "optimizer = getattr(optim, best_params[\"optimizer\"])(model.parameters(), lr=best_params[\"lr\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BeN9TEkuakW1"
      },
      "outputs": [],
      "source": [
        "# Entrenar modelo con los mejores par√°metros\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_batch)\n",
        "        loss = criterion(y_pred, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjbko-o7arJ8",
        "outputId": "0ea5d87a-b3ac-4396-c667-45a7c5772902"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Test Loss: 1.3872\n",
            "Mean Absolute Error (MAE): 0.2957\n",
            "Mean Squared Error (MSE): 0.1714\n",
            "Root Mean Squared Error (RMSE): 0.4140\n",
            "R¬≤ Score: 0.8189\n"
          ]
        }
      ],
      "source": [
        "# Evaluaci√≥n en test set\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        y_hat = model(X_batch)\n",
        "        test_loss += criterion(y_hat, y_batch).item()\n",
        "\n",
        "        # Guardar valores reales y predichos para m√©tricas\n",
        "        y_true.extend(y_batch.cpu().numpy().flatten())\n",
        "        y_pred.extend(y_hat.cpu().numpy().flatten())\n",
        "\n",
        "# Convertir listas a arrays de NumPy\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# Calcular m√©tricas\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "# Imprimir m√©tricas\n",
        "print(f\"Final Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"R¬≤ Score: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMTF4hv_cdT-",
        "outputId": "30433797-31f9-4212-f0ff-8dbc2f6471df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.5249 | Eval Loss: 1.3554 | Test Loss: 1.3872\n",
            "Train RMSE: 0.2447 | Eval RMSE: 0.4072 | Test RMSE: 0.4140\n",
            "Train R¬≤: 0.9401   | Eval R¬≤: 0.8534   | Test R¬≤: 0.8189\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, criterion, data_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            y_hat = model(X_batch)\n",
        "            total_loss += criterion(y_hat, y_batch).item()\n",
        "\n",
        "            y_true.extend(y_batch.cpu().numpy().flatten())\n",
        "            y_pred.extend(y_hat.cpu().numpy().flatten())\n",
        "\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    return total_loss, mae, mse, rmse, r2\n",
        "\n",
        "# Evaluaci√≥n en train, eval y test\n",
        "train_loss, train_mae, train_mse, train_rmse, train_r2 = evaluate_model(model, criterion, train_loader, device)\n",
        "eval_loss, eval_mae, eval_mse, eval_rmse, eval_r2 = evaluate_model(model, criterion, eval_loader, device)\n",
        "test_loss, test_mae, test_mse, test_rmse, test_r2 = evaluate_model(model, criterion, test_loader, device)\n",
        "\n",
        "print(f\"Train Loss: {train_loss:.4f} | Eval Loss: {eval_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Train RMSE: {train_rmse:.4f} | Eval RMSE: {eval_rmse:.4f} | Test RMSE: {test_rmse:.4f}\")\n",
        "print(f\"Train R¬≤: {train_r2:.4f}   | Eval R¬≤: {eval_r2:.4f}   | Test R¬≤: {test_r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcojEYPmhnkP"
      },
      "source": [
        "Over fitting Controlado!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "QzDYuFQTjyJH",
        "outputId": "46c218cf-d6ee-4c2a-a7c6-76478ca9596a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "importances = optuna.importance.get_param_importances(study)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(importances.keys(), importances.values())\n",
        "plt.xlabel(\"Hiperpar√°metros\")\n",
        "plt.ylabel(\"Importancia\")\n",
        "plt.title(\"Importancia de los hiperpar√°metros en LSTM\")\n",
        "plt.xticks(rotation=45, ha=\"right\")  # Rotar etiquetas del eje x para mejor legibilidad\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku4qPJmRiJ3C"
      },
      "source": [
        "Guardamos mejores par√°metros y m√©tricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo guardado en 'modelo_LSTM.pkl'\n",
            "M√©tricas y mejores hiperpar√°metros guardados en 'metricas_y_best_params_LSTM.json'\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "\n",
        "# 1. Crear un diccionario con las m√©tricas\n",
        "metrics = {\n",
        "    \"Train\": {\n",
        "        \"MSE\": train_mse,\n",
        "        \"RMSE\": train_rmse,\n",
        "        \"R^2\": train_r2,\n",
        "        \"MAE\": train_mae,\n",
        "        \"Loss\": train_loss\n",
        "    },\n",
        "    \"Val\": {\n",
        "        \"MSE\": eval_mse,\n",
        "        \"RMSE\": eval_rmse,\n",
        "        \"R^2\": eval_r2,\n",
        "        \"MAE\": eval_mae,\n",
        "        \"Loss\": eval_loss\n",
        "    },\n",
        "    \"Test\": {\n",
        "        \"MSE\": test_mse,\n",
        "        \"RMSE\": test_rmse,\n",
        "        \"R^2\": test_r2,\n",
        "        \"MAE\": test_mae,\n",
        "        \"Loss\": test_loss\n",
        "    }\n",
        "}\n",
        "\n",
        "# 2. Crear un diccionario con los mejores hiperpar√°metros\n",
        "results = {\n",
        "    \"metrics\": metrics,\n",
        "    \"best_params\": best_params\n",
        "}\n",
        "\n",
        "# 3. Guardar todo en un archivo JSON\n",
        "json_path = \"metricas_y_best_params_LSTM.json\"\n",
        "with open(json_path, \"w\") as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "# 4. Guardar el modelo en un archivo .pkl\n",
        "torch.save(model, \"modelo_LSTM.pkl\")\n",
        "\n",
        "# 5. Guardar columnas de X_train\n",
        "with open('LSTM_X_train_columns.txt', 'w') as f:\n",
        "    for col in X_train_columns:\n",
        "        f.write(f\"{col}\\n\")\n",
        "\n",
        "print(f\"Modelo guardado en 'modelo_LSTM.pkl'\")\n",
        "print(f\"M√©tricas y mejores hiperpar√°metros guardados en '{json_path}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--C√≥digo Fer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "PHjHyxB_jwfz",
        "outputId": "fe3209c8-1f7f-4f1e-e13f-c978c479df75"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 538,\n  \"fields\": [\n    {\n      \"column\": \"y_true\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 53,\n        \"samples\": [\n          1.4000000953674316,\n          3.799999952316284,\n          2.200000047683716\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"y_pred\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 538,\n        \"samples\": [\n          0.8662731051445007,\n          5.112593650817871,\n          3.019166946411133\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "results_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-e1142a2b-5735-4768-af18-c906debbb607\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y_true</th>\n",
              "      <th>y_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.3</td>\n",
              "      <td>1.877265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.1</td>\n",
              "      <td>2.864897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6.0</td>\n",
              "      <td>5.569900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.9</td>\n",
              "      <td>5.844567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.1</td>\n",
              "      <td>3.250195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>533</th>\n",
              "      <td>1.4</td>\n",
              "      <td>1.678861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>534</th>\n",
              "      <td>5.0</td>\n",
              "      <td>4.418911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>535</th>\n",
              "      <td>6.1</td>\n",
              "      <td>5.996844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>536</th>\n",
              "      <td>2.0</td>\n",
              "      <td>1.484212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>537</th>\n",
              "      <td>1.3</td>\n",
              "      <td>2.043751</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>538 rows √ó 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1142a2b-5735-4768-af18-c906debbb607')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e1142a2b-5735-4768-af18-c906debbb607 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e1142a2b-5735-4768-af18-c906debbb607');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3470c46e-3393-4846-a786-b43c107d3627\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3470c46e-3393-4846-a786-b43c107d3627')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3470c46e-3393-4846-a786-b43c107d3627 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_0bb972ae-5220-4077-bda3-71aa0e0abb16\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_0bb972ae-5220-4077-bda3-71aa0e0abb16 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "     y_true    y_pred\n",
              "0       1.3  1.877265\n",
              "1       4.1  2.864897\n",
              "2       6.0  5.569900\n",
              "3       5.9  5.844567\n",
              "4       4.1  3.250195\n",
              "..      ...       ...\n",
              "533     1.4  1.678861\n",
              "534     5.0  4.418911\n",
              "535     6.1  5.996844\n",
              "536     2.0  1.484212\n",
              "537     1.3  2.043751\n",
              "\n",
              "[538 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "y_true_denormalized = scaler_y.inverse_transform(y_true.reshape(-1, 1))\n",
        "y_pred_denormalized = scaler_y.inverse_transform(y_pred.reshape(-1, 1))\n",
        "\n",
        "results_df = pd.DataFrame({'y_true': y_true_denormalized.flatten(), 'y_pred': y_pred_denormalized.flatten()}) # Use denormalized values\n",
        "\n",
        "display(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R03SkuVjlySK",
        "outputId": "fc1a5d51-a3cc-4eea-ed17-36e26a1e2337"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(538,)"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBbpgr5Ml-cO",
        "outputId": "bc9c9f26-3a06-4739-ecd3-4f24444d009e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(538,)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_true.shape"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "proyecto_keepcoding",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
